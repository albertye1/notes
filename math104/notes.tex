\documentclass{book}
\usepackage{michael}

\title{Math 104: Real Analysis}
\author{Albert Ye}
\date{\today}

\begin{document}
\maketitle
\mytoc 
\chapter{Week 1}
\section{Lecture 1}
\subsection{Logic and Sets}
For clauses $p$, $q$: we have $p \land q$, $p \lor q$, $\lnot p$. These are \textit{and}, \textit{or}, \textit{not}; respectively. 

Moreover, we have $p \implies q$ meaning that $q$ is true if $p$ is true. Moreover, we have $p \iff q$ meaning that $p$ is true if $q$ is true and $q$ is true if $p$ is true.

Other terminology: $:=$ is a definition, $\forall$ is for all, $\exists$ is exists, $a \in A$ means that element $a$ is in the set $A$, $a \notin A$ means that element $a$ isn't in the set $A$.

For sets, we have $\subset, =, \subseteq$ to determine subset and equality relations. Moreover, we have $\cap, \cup$ to represent union and intersections of sets. There is also $A \setminus B$ to denote everything in $A$ but not $B$, and we have $A^C$ to denote every element not in $A$.

\begin{thm}[DeMorgan's Laws]
    Let $A$ and $B$ be sets. 
    \begin{enumerate}[label = (\alph*)]
        \item $(A \cup B)^C = A^C \cap B^C$
        \item $(A \cap B)^C = A^C \cup B^C$
        \item $A \setminus (B \cap C) = (A \setminus B) \cup (A \setminus C)$
        \item $A \setminus (B \cup C) = (A \setminus B) \cap (A \setminus C)$
    \end{enumerate}
\end{thm}

\subsection{Indexed Sets}
Let $\Lambda$ be a set and suppose for each $a \in \Lambda$ there is a set $A_a$. The set $\{A_a : a \in \Lambda\}$ is called a \vocab{collection of sets indexed by $\Lambda$}. In this case, $\Lambda$ is called the \vocab{indexing set} for this collection.

\[\bigcup_{a \in A} = \{x | x \in A_a \textrm{ for some } a \in A\}\]
\[\bigcap_{a \in A} = \{x | x \in A_a \textrm{ for all } a \in A\}.\]

We can generalize DeMorgan's laws to indexed collections: 
\begin{thm}[Generalized DeMorgan]
    If $\{B_a : a \in \Lambda\}$ is an indexed collection of sets and $A$ is a set, then 
    \[A \setminus \bigcup_{a \in \Lambda} B_a = \bigcap_{a \in \Lambda} (A \setminus B_a),\]
    \[A \setminus \bigcap_{a \in \Lambda} B_a = \bigcup_{a \in \Lambda} (A \setminus B_a).\] 
\end{thm}

\subsection{Set of Natural Numbers}
We set $\NN$ to be all positive integers, $\ZZ$ to be all integers, and $\NN_0$ to be all nonnegative integers.

\begin{definition}[Peano Axioms]
    \begin{enumerate}
        \item $1 \in \NN$.
        \item If $n \in \NN$, then $n + 1 \in \NN$. We'll call this the \vocab{successor}.
        \item $1$ is not the successor of any element
        \item If $n, m \in NN$ have the same successor, then $n = m$. 
        \item (Induction) If $S \subseteq \NN$ with the properties $1 \in S$ and $n \in S \implies n + 1 \in S$, then $S = \NN$. This becomes induction when we have $S$ as the set of elements where a certain property holds.
    \end{enumerate}
\end{definition}

So, for induction, we have a base case where we have $P_0$ or $P_1$ or some starting value. And then, we have induction that proves that $P_k$ being true implies $P_{k + 1}$ is true. Then it dominoes over.

Remember that we didn't prove that $P_{n + 1}$ is true, but rather that it can be implied from $P_n$.

\subsection{Set of Rational Numbers}
We define $\QQ$, the set of rational numbers, by $\QQ := \{\frac{m}{n} | m, n \in \ZZ, n \neq 0\}$.

\begin{remark}
    $\QQ$ contains all terminating decimals.
\end{remark}

\begin{remark}
    If $\frac{m}{n} \in \QQ$ and $r \in \ZZ \setminus \{0\}$, then $\frac{m}{n} = \frac{rm}{rn}$, so we assume that $m, n$ are coprime usually.
\end{remark}

\begin{definition}[Field Axioms]
    Remembering these is now an exercise for the reader.
\end{definition}

We see that the set of rational numbers with addition and multiplication is a field. Going through the axioms is left as an exercise to the reader.

\newpage
\section{Lecture 2}
\subsection{Ordered Sets}
\begin{defn}[Ordered Set]
    We define an \vocab{ordered set} to be a set $S$ with an order satisfying the following criteria:
    \begin{enumerate}
        \item $\forall \alpha, \beta \in S$, either $\alpha < \beta, \alpha = \beta, \alpha > \beta$. 
        \item $\alpha < \beta, \beta < \gamma \implies \alpha < \gamma$.
        \item $\alpha \leq \beta \implies \alpha + \gamma \leq \beta + \gamma$
        \item $\alpha \leq \beta, \gamma \geq 0 \implies \alpha \gamma \leq \beta \gamma$
        \item $\alpha \leq \beta, \beta \leq \alpha \implies \alpha = \beta$
    \end{enumerate}
\end{defn}

A set that is a field and an ordered set can be called an \vocab{ordered field}.

\subsection{Defects of $\QQ$}
\begin{thm}[Irrationality of $\sqrt 2$]
    There is no $\alpha$ such that $\alpha^2 = 2$.
\end{thm}

\begin{proof}
    Suppose for the sake of contradiction that there is $\alpha \in \QQ$ such that $\alpha^2 = 2$. We see that $\alpha = \frac{m}{n}$ for some $m, n \in \ZZ$ such that $\gcd(m, n) = 1$.

    Since $a^2 = 2$, we have $\frac{m^2}{n^2} = 2$, implying that $m^2 = 2n^2$, or $2 | m^2$. As $2$ is prime, we see that $2 | m \implies m = 2p$ for some integer $p$. Then, $(2p)^2 = 2n^2 \implies 4p^2 = 2n^2 \implies n^2 = 2p^2$. Once again, we have that $2 | n$ from above, so $m, n$ are both even. This contradicts the claim that $m, n$ are coprime, so $\sqrt 2$ cannot be expressed in a rational form. 
\end{proof}

This motivates the concept of incompleteness.

\begin{defn}[Incompleteness]
    Let $S$ be an ordered set, and let $A \subseteq S$. 
    \begin{enumerate}
        \item An element $\beta \in S$ is an \vocab{upper bound} for $A$ if $\alpha \leq \beta, \forall \alpha \in A$. 

        Then, we say that $A$ is \vocab{bounded above}.
        \item An element $\beta \in S$ is a \vocab{lower bound} for $A$ if $\alpha \geq \beta, \forall A \in A$. 
        
        Then, we say that $A$ is \vocab{bounded below}.
    \end{enumerate}
\end{defn}

\begin{defn}[Supremum]
    Suppose $S$ is an ordered set and $A \in S$ is bounded above. Suppose $\exists B \in S$ such that:
    \begin{enumerate}
        \item $\beta$ is an upper bound for $A$.
        \item If $r$ is another upper bound, then $r \geq \beta$.
    \end{enumerate}
    Then, we will call $\beta$ the least upper bound, or the \vocab{supremum} ($\sup$) of $A$.
\end{defn}

The greatest lower bound is then called the \vocab{infimum} ($\inf$) of $A$.

\begin{remark}
    Supremum and infimum may not exist or belong to $A$.
\end{remark}

\begin{defn}[Completeness]
    An ordered set $S$ is said to have the least upper bound property, or \vocab{completeness}, if every upper bounded set has a supremum in $S$.
\end{defn}

\subsection{Real Numbers}
\begin{thm}[Real Numbers]
    There is a unique ordered field $(\RR, +, \cdot, \leq)$ that has the following properties:
    \begin{enumerate}
        \item Completeness.
        \item $\QQ \subseteq \RR$ is an ordered subfield; i.e., $(+, \cdot, \leq)$ restricted to $\QQ$ are the usual $(+, \cdot, \leq)$ on $\QQ$.
    \end{enumerate}
\end{thm}

Lecturer says we will be using the result, and not the proof.

There's another theorem with properties of real number arithmetic but honestly I'm too lazy to write it as of now so you'll see it later.

\subsection{Consequences of the Completeness Axiom}

\subsubsection{Existence of Infimum}
\begin{thm}
    Let $E \subset \RR$ be a set bounded below. Then $\inf E$ exists in $\RR$.
\end{thm}
    
\begin{proof}
    Define $-E$ to be $\{-x | x \in E\}$. Then, we see that $-E$ must be bounded above, implying that it must have a supremum by definition of completeness. Then, we let $\sup -E = \beta$. 

    Then, for any $\alpha$ such that $x \geq \alpha \forall x \in E$, then we have $y \leq -\alpha \forall y \in E$. We see that $-\alpha \geq \beta$ by definition of supremum, so $\alpha \leq -\beta$. As a result, $\inf E = -\beta$.
\end{proof}

\newpage
\section{Lecture 3}
\subsection{Archimedean Property}
\begin{thm}[Archimedean Property]
    If $a > 0$ and $b > 0$, then for some positive integer $n$, we have $na > b$.
\end{thm}

\begin{proof}
    Assume that the Archimedean Property fails. Then, for all positive integers $n$, we have $na < b$ for some positive $a$ and $b$. 

    Now, let's observe $S = \{na | n \in \NN\}$. Then, let $b = \sup S$, which must exist by completeness.

    Consider $b - a$. Since $b$ is a supremum and $a$ is positive, then $\exists s \in S$ such that $s > b - a$. However, $a + s$ must also be in $S$ by definition, and $a + s > a + (b - a) = b$, contradicting the claim that $b = \sup S$.
\end{proof}

\begin{corollary}
    \begin{enumerate}
        \item If $a > 0$, then $\exists n \in \NN$ such that $\frac{1}{n} < a$.
        \item If $b > 0$, then $\exists n \in \NN$ such that $b < n$.
    \end{enumerate}
\end{corollary}

\begin{thm}[Density of $\QQ$]
    If $a, b \in \RR$ and $a < b$, then $\exists r \in \QQ$ such that $a < r < b$.
\end{thm}

\begin{proof}
    This is an exercise for the reader. Till I fill the proof in.
\end{proof}

\begin{thm}[Existence of $n$th roots]
    Given any $\alpha \in \RR$, $\alpha > 0$, and any $n \in \NN$, there's a $\beta \in \RR$ s.t. $\beta^n = \alpha$.
\end{thm}

\begin{proof}
    This is an exercise for the reader until I feel like writing more about this.
\end{proof}

\begin{corollary}
    \begin{enumerate}
        \item $b_1, b_2 > 0$ s.t. $b_1^n = b_2^n$. Then, $b_1 = b_2$.
        \item If $a, b > 0$, then $\sqrt[n]{ab} = \sqrt[n]{a}  \sqrt[n]{b}$.
    \end{enumerate}
\end{corollary}

\subsection{(Gates to) Infinity}
We define the set of \vocab{extended reals} to be $\RR^* = \RR \cup \{-\infty\} \cup \{\infty\}$, with the extended order $-\infty < \alpha < \infty$, $\forall \alpha \in \RR$.

Then, $\infty$ is an upper bound for any $E \subset \RR$ and $-\infty$ is a lower bound for any $E \subset \RR$. 

We can extend the definition of $\sup$ and $\inf$ such that
\begin{itemize}
    \item $\sup E = \inf$ if $E$ is not bounded above, and 
    \item $\inf E = -\inf$ if $E$ is not bounded below.
\end{itemize}

Note that $\RR^*$ does not form a field. As a result, we cannot apply a theorem or exercise stated for real numbers to $\infty$, $-\infty$. This set doesn't have an algebraic structure.

We also denote unbounded intervals using $-\infty, \infty$ instead of real numbers.

\begin{remark}
    Let $S$ be any nonempty subset of $\RR$. The symbols $\sup S$ and $\inf S$ always make sense. If $S$ is bounded above, then $\sup S$ is a real; otherwise, it is $+\infty$. Same logic applies to lower bounds and $-\infty$.

    Moreover, the statement $\inf S \leq \sup S$ also always makes sense.
\end{remark}

\chapter{Week 2}
\section{Lecture 4}
\subsection{Limits of Sequences}
\begin{defn}[Sequence]
    A \vocab{sequence} is a function $S$ whose domain is a set of the form $\{n \in \ZZ : n \geq m\}$; $m$ is usually $1$ or $0$.

    Or, a sequence is an infinite list of real numbers.
\end{defn}

Note that we must be careful in ensuring that we have $\ldots$ at the end of our list for repeating sequences, to make clear that our sequence goes to infinity.

Given a sequence $S_1, S_2, \ldots$, we want to figure out what happens to $S_n$ as $n \to \infty$. 

\begin{ex}
    $S_n = \frac{1}{\sqrt n}, n \in \NN$. The terms seem to "approach" zero.
\end{ex}

\begin{ex}
    $S_n = (-1)^n, n \geq 0$. The sequence jumps around, and it appears to not approach any single value.
\end{ex}

Intuitively: $\lim_{n \to \infty} S_n = S$ means that as $S$ gets large, then $S_n$ goes to $S$.

\subsubsection{Epsilon-Delta}

\begin{defn}[Formal Definition of Convergence]
    A sequence $S_n$ of real numbers is said to \vocab{converge} to the real number $S$ if:

    \[\forall \epsilon > 0, \exists N = N(\epsilon) \textrm{s. t. } n > N \implies |S_n - S| \leq \epsilon.\]
\end{defn}
So, no matter how small $\epsilon$ is, there is a threshold $N$ s.t. once you have $n > N$, then you can guarantee that $S_n$ is at most $\epsilon$ away from $S$.

\begin{defn}[Limits]
    If $S_n$ converges to $S$, we will write that \[\lim_{n \to \infty} S_n = S, \text{ or } S_n \to S.\] 

    The number $S$ is called the \vocab{limit} of the sequence $(S_n)$. 

    A sequence that doesn't converge to any set number is said to \vocab{diverge}.
\end{defn}

\begin{remark}
    \begin{enumerate}
        \item The threshold $N$ in the first definition can be treated as a positive integer by the Archimedean Property.
        \item It's traditional to use $\epsilon$ and $\delta$ in situations where the interesting values are small positive values.
        \item The first definition is an infinite number of statements, one for each $\epsilon$.
    \end{enumerate}
\end{remark}

Also, usually $N$ depends on $\epsilon$, usually with an inverse relationship.

\subsection{Proving Limits}
\begin{ex}
    $\lim \frac{1}{\sqrt n} = 0$.
\end{ex}

We see that $|S_n - S| = |\frac{1}{\sqrt n} - 0| = |\frac{1}{\sqrt n}| = \frac{1}{\sqrt n}$. We claim that this is less than $\epsilon$ for some $n$.

We have that $\frac{1}{n} < \epsilon^2 \implies n > \frac{1}{\epsilon^2}$. However, as $n$ is unbounded, we see that there is some $n \in \NN$ such that $n > \frac{1}{\epsilon^2}$. 

But this isn't a rigorous mathematical proof.

\begin{proof}
    Let's set $N = \frac{1}{\epsilon^2}$. We claim that $|\frac{1}{\sqrt n} - 0| \leq \epsilon$. Setting $n > \frac{1}{\epsilon^2}$ implies that $\frac{1}{n} < \epsilon^2$. Therefore, we have $|]frac{1}{\sqrt n} - 0| = |\frac{1}{\sqrt n}| = \frac{1}{\sqrt n} < \epsilon$. Therefore, our sequence $S_n = \frac{1}{\sqrt n}$ must converge to $0$.
\end{proof}

\begin{ex}
    $\lim_{n \to \infty} \frac{2n + 4}{5n + 2} = \frac{2}{5}$.
\end{ex}

We see that \begin{align*}
    |S_n - S| &= |\frac{2n + 4}{5n + 2} - \frac{2}{5}| \\
    &= |\frac{10n + 20 - (10n + 4)}{5(5n + 2)}|\\
    &= |\frac{16}{25n + 10}| \\
    &= \frac{16}{25n + 10}.
\end{align*}

We want this value to be less than $\epsilon$, so we have $\frac{16}{25n + 10} < \epsilon \implies 25n + 10 > \frac{16}{\epsilon} \implies n > \frac{16 - 10 \epsilon}{25\epsilon}$.

\begin{proof}
    We set $N$ to be $\frac{16 - 10 \epsilon}{25 \epsilon}$ for all $\epsilon > 0$. Then, we claim that for all $n > N$, $|\frac{2n + 4}{5n + 2} - \frac{2}{5}| < \epsilon$.

    We see from above that this is equivalent to $\frac{16}{25n + 10} < \epsilon$. However, we know that $n > N \implies n > \frac{16 - 10 \epsilon}{25 \epsilon}$. Then, we have 
    \[ \frac{16}{25n + 10} < \frac{16}{25\left(\frac{16 - 10 \epsilon}{25 \epsilon} \right) + 10} = \frac{16}{\frac{16 - 10 \epsilon}{\epsilon} + 10} = \frac{16}{\frac{16}{\epsilon}} = \epsilon.\]

    Therefore, our sequence $S_n = \frac{2n + 4}{5n + 2}$ does indeed converge to $\frac{2}{5}$.
\end{proof}

We \textit{must} write the formal proof, because all we do in the first part is find a potential threshold $N$, and not prove that it is valid.

\newpage
\section{Lecture 5}
\subsection{Limits Theorem for Sequences}
We start by showing that limits are unique.
\begin{thm}
    If $\lim_{n \to \infty} S_n = s$ and $\lim_{n \to \infty} S_n = t$, then $s = t$.
\end{thm}

\begin{proof}
    We first reframe this in terms of $\epsilon$. Then, we see that for each $\epsilon > 0$, there exists an $N_1, N_2$ such that $n > N_1 \implies |S_n - s| < \frac{\epsilon}{2}$. Also, $n > N_2 \implies |S_n - t| < \frac{\epsilon}{2}$. Now, we consider $N = \max(N_1, N_2)$. 

    From the Triangle Inequality, we see that for all $n > N$, $|s - t| \leq |S_n - s| + |S_n - t| < \frac{\epsilon}{2} + \frac{\epsilon}{2}$. As a result, $\forall n > N, |s - t| < \epsilon$. Therefore, $|s - t| < \epsilon$ for all $\epsilon > 0$, implying that $|s - t| = 0$, or $s = t$.
\end{proof}

\begin{defn}[Bounded Sequence]
    A sequence $S_n$ is said to be \vocab{bounded} if its set of values $\{S_n : n \in \NN\}$ is a bounded set.
\end{defn}

\begin{thm}
    Convergent sequences are bounded.
\end{thm}

\begin{proof}
    Let $S_n$ be a convergent sequence, and let $S = \lim_{n \to \infty} S_n$. Take $\epsilon = 1$, then we obtain $N \in \NN$ such that \[\forall n > N, |S_n - S| < 1.\] 

    By the triangle inequality, for all $n > N$, we have $|S_n| = |S_n - s + S| \leq |S_n - S| + |S| < 1 + |S|$.

    Now, let $M = \max\{|S_1|, |S_2|, \ldots, |S_N|, |S| + 1$. Then, we see that $|S_n| \leq M, \forall n \in \NN$. Therefore, $(S_n)$ is a bounded sequence.
\end{proof}

\begin{remark}
    The converse of the above theorem is not true. Consider $(-1, 1, \ldots)$. Some condition must be added.
\end{remark}

For cases where we need to prove a sequence is bounded, we should set $\epsilon$ to $1$ and try to find a resulting upper bound.

\subsection{Limit Laws}
\begin{thm}
    If $S_n \to S$ and $k \in \RR$, then $k S_n \to k S$. 

    That is, $\lim kS_n = k \lim S_n$.
\end{thm}

\begin{proof}
    Let $k \neq 0$, and let $\epsilon > 0$.

    For all $n > N$, we have $|S_n - S| < \frac{\epsilon}{|k|}$. We want to show that $|kS_n - kS| < \epsilon$ for $n > N$ as well.

    Then, \begin{align*}
        n > N &\implies |k| \cdot |S_n - S| < \epsilon \\
        n > N &\implies |k S_n - k S| < \epsilon,
    \end{align*}

    and we are done.
\end{proof}

\begin{thm}
    If $s_n \to s$ and $t_n \to t$, then $(s_n + t_n) \to s + t$.
\end{thm}

\begin{proof}
    We let $\epsilon > 0$.

    We know that there exists $N_1, N_2 \in \NN$ such that for $n > N_1$, we have $|s_n - s| < \epsilon / 2$, and for $n > N_2$ we have $|t_n - t| < \epsilon / 2$. 

    Then, let $N = \max(N_1, N_2)$. then, $\forall n > N$ we have $|s_n - s| + |t_n - t| < \epsilon$. We have that that $|s_n - s| + |t_n - t| \geq |s_n + t_n - s - t| = |(s_n + t_n) - (s + t)|$, so it follows that $|(s_n + t_n) - (s + t)| < \epsilon / 2 + \epsilon / 2 = \epsilon$.
\end{proof}

\begin{thm}
    If $s_n \to s$ and $t_n \to t$, then $s_nt_n \to st$.
\end{thm}

\begin{proof}
    I'm gettin lazy 
\end{proof}

\begin{lemma}
    If $s_n \neq 0$ for all $n$ and $s_n \to s \neq 0$, then $\frac{1}{s_n} \to \frac{1}{s}$. 
\end{lemma}

\begin{thm}
    $\lim \frac{t_n}{s_n} = \frac{\lim t_n}{\lim s_n}$.
\end{thm}

\begin{thm}
    We have the following limit laws:
    \begin{enumerate}[label=(\alph*)]
        \item $\lim_{n \to \infty} \frac{1}{n^p} = 0$ for $p > 0$.
        \item $\lim_{n \to \infty} a^n = 0$ if $|a| < 1$
        \item $\lim_{n \to \infty} n^\frac{1}{n} = 1$.
        \item $\lim_{n \to \infty} a^{\frac{1}{n}} = 1$ if $a > 0$.
    \end{enumerate}
\end{thm}

The proof uses both the binomial and squeeze theorems. Proof happens in the next lecture.

\begin{thm}[Squeze Theorem]
    Let $(a_n), (b_n), (c_n)$ be sequences such that $a_n \leq b_n \leq c_n$. Then,
    \begin{enumerate}
        \item If $a_n \to A$, $b_n \to B$, $c_n \to C$, then $A \leq B \leq C$. 
        \item If $\lim a_n = \lim c_n = L$, then $\lim b_n$ exists and is equal to $L$.
    \end{enumerate}
\end{thm}

\newpage
\section{Lecture 6}
\subsection{Proofs of Limit Laws}
\begin{enumerate}[label = (\alph*)]
    \item fill out later
    \item fill out later 
    \item fill out later 
    \item fill out later 
\end{enumerate}
\subsection{Infinite Limits}
\begin{defn}[Divergence]
    There are two cases for divergence. 

    $\lim_{n \to \infty} s_n = \infty$ if for each $M > 0$, there exists an $N$ such that $n > N \implies s_n > M$.

    $\lim_{n \to \infty} s_n = -\infty$ if for each $M < 0$, there exists an $N$ such that $n > N \implies s_n < M$.

    From now on, we'll say that a limit \textbf{converges}, \textbf{diverges to }$+\infty$, or \textbf{diverges to }$-\infty$.
\end{defn}

We now say that $(s_n)$ \vocab{has a limit}, or the limit \vocab{exists}, if it is convergent or divergent.

\begin{remark}
    Many sequences don't have limits of $+\infty$, $-\infty$, even when unbounded.
\end{remark}

\begin{ex}
    Show that $\lim_{n \to \infty} \sqrt{n - 5} + 3 = +\infty$.
\end{ex}

\begin{proof}
    Our first step is to find $N$. We have 
    \begin{align*}
        \sqrt{n - 5} + 3 > M &\implies \sqrt{n - 5} > M - 3\\
        &\implies n - 5 > (M - 3)^2 \\
        &\implies n > (M - 3)^2 + 5.
    \end{align*}

    Thus, we let $N = (M - 3)^2 + 5.$

    Next, we let $M > 0$ be given, and let $N = (M - 3)^2 + 5$ s.t. $n > N \implies \sqrt{n - 5}+3 > \sqrt{(M - 3)^2 + 5 - 5} + 3 = M-3 + 3 = M$.
\end{proof}

\begin{thm}
    Suppose $s_n \to +\infty$, $t_n \to t$. Then, $s_nt_n \to +\infty$. This is true for any $t > 0$ or $t = +\infty$.
\end{thm}

\begin{proof}
    Let $M > 0$. Select a real number $m$ such that $0 < m < \lim t_n$. Regardless of if $t_n \to \infty$, there exist an $N_1$ such that $n > N_1 \implies t_n > m$.

    $S_n \to +\infty$ implies that there exists an $N_2$ such that $n > N_2 \implies s_n > \frac{M}{m}$. Now, let $N = \max(N_1, N_2)$. We see that $n > N \implies s_nt_n > \frac{M}{m} \cdot m = M$.
\end{proof}

\begin{thm}[Duality]
    If $s_n > 0 \forall n$, then \[s_n \to +\infty \iff \frac{1}{s_n} \to 0.\]
\end{thm}
\begin{proof}
    For the forward direction, let $\epsilon > 0$ be given. Let $M = \frac{1}{\epsilon}$. Since $s_n \to \infty$, then there must exist an $N$ such that $\forall n > N \implies s_n > M = \frac{1}{\epsilon}$. 

    But then, for that same $N$, if $n > N$, we have $|\frac{1}{s_n} - 0| = \frac{1}{s_n} < \frac{1}{M} = \epsilon$ from the previous paragraph. This implies that $|\frac{1}{s_n} - 0| = 0$, or the limit is $0$. \qed

    For the backwards direction, we let $M > 0$ be given. Let $\epsilon = \frac{1}{M}$. Then, since $\frac{1}{s_n} \to 0$, there exists an $N$ such that $\forall n > N$, $|\frac{1}{s_n}| = \frac{1}{s_n} < \epsilon = \frac{1}{M}$.

    But for the same $N$, we have $n > N \implies S_n > \frac{1}{\epsilon} = \frac{1}{\frac{1}{M}} = M$, so the limit must tend to infinity.
\end{proof}

\section{Lecture 7}
\subsection{Monotonic Sequences}
\begin{defn}[Monotonicity]
    A sequence $(s_n)$ is increasing if $s_{n + 1} \geq s_n$ for all $n$, and decreasing of $s_{n + 1} \leq s_n$ for all $n$. If either of these describes a sequence, that sequence is \vocab{monotonic}.
\end{defn}

The following theorem shows why we care about monotonic sequences:

\begin{thm}
    Let $(s_n)$ be a sequence. 

    \begin{enumerate}
        \item If $(s_n)$ is increasing and bounded above then $(s_n)$ converges.
        \item If $(s_n)$ is decreasing and bounded below then $(s_n)$ converges.
    \end{enumerate}
\end{thm}

\begin{corollary}
    All monotonic sequences that are bounded converge.
\end{corollary}

\begin{thm}
    \begin{enumerate}
        \item An increasing unbounded sequence diverges to $+\infty$.
        \item A decreasing unbounded sequence diverges to $-\infty$.
    \end{enumerate}
\end{thm}

\begin{proof} $\qquad$
    \begin{enumerate}
        \item We claim for the sake of contradiction that $(s_n)$ is unbounded but does not diverge. Then, for some $M > 0$, there exists $s_n$ such that $s_n$
        \item Similar argument.
    \end{enumerate}
\end{proof}

\subsection{Cauchy Sequences}

\begin{defn}
    A sequence $(s_n)$ of real numbers is called a \vocab{Cauchy sequence} if \[\forall \epsilon > 0 \exists N \in \NN \textrm{ s.t. } n, m > N \implies |s_n - s_m| < \epsilon.\] 
    
    In other words, the terms get closer and closer to each other.
\end{defn}

\begin{lemma}
    If a sequence is convergent, it's a Cauchy sequence.
\end{lemma}

\begin{proof}
    Suppose $(s_n)$ is convergent, say $s_n \to s$. Then, let $\epsilon > 0$ be given. Then, there must exist an $N$ such that $n > N \implies |s_n - s| < \epsilon/2$. However, for any $m > N$ we also have $s_m - s < \epsilon/2$. Then, it follows that $|s_n - s| + |s_m - s| < \epsilon$. Moreover, from the Triangle Inequality, $|s_n - s_m| = |s_n - s + s - s_m| \leq |s_n - s| + |s_m - s|$, so we have $|s_n - s_m| < \epsilon$.
\end{proof} 

And what about the other direction?
\begin{lemma}
    Cauchy sequences are bounded.
\end{lemma}

The proof is similar for that of the proof that convergent sequences are bounded.

\begin{proof}
    Let $(s_n)$ be a Cauchy sequence. Then, taking $\epsilon = 1$, $\exists N \in \NN$ such that $m, n > N \implies |s_n - s_m| = 1$. 
\end{proof}

We see that for bounded sequences that don't converge, taking the supremum of all elements $n > N$ for each $N$ leads to a convergent sequence. So even when there's no limit, we can still find a way to get a sequence with a limit.

\begin{defn}[$\lim \sup$]
    Let $(s_n)$ be a sequence in $\RR$. We define,
    \[\lim \sup s_n = \lim_{n \to \infty} \sup \{s_n : n > N\}.\] 

    $\lim \inf$ can be defined similarly.
\end{defn}

\chapter{Week 3}
\section{Lecture 8}
\subsection{Subsequence}
\begin{defn}[Subsequence]
    A \vocab{subsequence} of $(s_n)$ is a sequence of the form: $(t_k)$, for $k \in \NN$ where for every $k$ $\exists$ an $n_k \in \NN$ such that $t_n = s_{n_k}$.
\end{defn}

\begin{thm}
    Let $(s_n)$ be a sequence.
    \begin{enumerate}[label = (\roman*)]
        \item If $t \in \RR$, there exists a subseq. of $(s_n) \to t$, if and only if the set $\{n \in \NN : |s_n - t| < \epsilon\}$ is infinite for all $\epsilon > 0$.
        \item If the sequence $(s_n)$ isn't bounded above, it has a subseq. with limit $+\infty$.
        \item The same thing, but for $(s_n)$ not bounded below and with limit $-\infty$.
    \end{enumerate}
\end{thm}

\begin{ex}
    $(s_n)$ is a sequence with positive numbers such that $\inf \{ S : n \in \NN \} = 0$. Show that this sequence must have a subsequence converging to $0$, monotonically.
\end{ex}

\begin{proof}
    We claim that $\{n \in \NN : |s_n - 0| \le \epsilon\}$ is infinite for all $\epsilon > 0$. This must be true, because for some otherwise, for some $\epsilon_0 > 0$, there are an infinite number of real numbers $\epsilon$ for which $\epsilon_0 > \epsilon > 0$. Therefore, the size of each described set must be infinite for the infimum of $S$ to be $0$.
\end{proof}

\begin{thm}
    If $(s_n)$ converges, then every subsequences converges to the same limit.
\end{thm}
 
\begin{proof}
    Let $(s_{n_k})$ denote a subsequence of $(s_n)$; we see that for all $k \in \NN$, $n_k \geq k$. We know that for any $\epsilon > 0$ and $\lim s_n = S$, there is an $N$ such that $n > N \implies |s_n - S| < \epsilon$. Then, $k > N \implies n_k > N \implies |s_{n_k} - S| < \epsilon$.

    Therefore, $\lim_{k \to \infty} s_{n_k} = S$.
\end{proof}

\begin{thm}
    Every sequence $(S_n)$ has a monotonic subsequence.
\end{thm}

\begin{proof}
    Let the $n$th term be "dominant" if it is greater than all following terms.

    Then, we have two options.
    \begin{enumerate}
        \item There are infinite dominant terms, which means taht all dominant terms form a decreasing subsequence.
        \item There are finite dominant terms, in which case we look for a value $n_1$ such that $S_{n_1}$ is larger than the largest dominant term. Then, $S_{n_1}$ is not the largest term for all $n > n_1$, so there must exist $n_2 > n_1$ such that $S_{n_2} \geq S_{n_1}$. Replacing $1$ and $2$ with $i$ and $i+1$ gives us the same results, so we are done by induction.
    \end{enumerate}
\end{proof}

\begin{thm}[Bolzano-Weierstrass]
    Every bounded sequence has a convergent subsequence.
\end{thm}

\begin{proof}
    Every bounded sequence must have a monotonic subsequence, which must converge because it is bounded.
\end{proof}

\begin{defn}[Subsequential Limit]
    A \vocab{subsequential limit} is any real number or infinity that is the limit of some subsequence of $(s_n)$.
\end{defn}

When a sequence has limit $s$, then all subsequences have limit $s$, so $\{s\}$ is the set of subsequential limits. This becomes interesting only when there are no limits.

\begin{thm}
    Let $(s_n)$ be any sequence. There exists a monotonic subsequence whose limit is $\lim \sup s_n$, and a monotonic subsequence whose limit is $\lim \inf s_n$.
\end{thm}

\begin{thm}
    Let $(s_n)$ be any sequence in $\RR$, and let $S$ denote the set of subsequential limits of $(s_n)$.

    \begin{enumerate}[label=(\roman*)]
        \item $S$ is nonempty.
        \item $\sup S = \lim \sup s_n, \inf S = \lim \inf s_n$.
        \item $\lim s_n$ exists iff $S$ has exactly one element, namely $\lim s_n$.
    \end{enumerate}
\end{thm}

\begin{thm}
    Let $S$ denote the set of subsequential limits of a sequence $(s_n)$. Suppose $(t_n)$ is a sequence in $S \cap \RR$ and that $t = \lim t_n$. Then, $t$ belongs to $S$.
\end{thm}

\subsection{$\lim \sup$ and $\lim \inf$ again}
Recall the definitions for $\lim \sup$ and $\lim \inf$:
\[\lim \sup s_n = \lim_{N \to \infty} \sup \{s_n : n > N\} = \sup S,\] 
\[\lim \inf s_n = \lim_{N \to \infty} \inf \{s_n : n > N\} = \inf S.\]

\begin{thm}
    If $(s_n)$ converges to a positive real number $s$ and $(t_n)$ is any other sequence, then \[\lim \sup s_nt_n = s \cdot \lim \sup t_n.\]

    We define $s \cdot \infty = \infty$.
\end{thm}

The hypothesis $s > 0$ cannot be extended to $s = 0$. Consider $s_n = \frac{1}{n}, t_n = -n^2$. Then, the LHS is $\infty$ and the RHS is $0 \cdot (-\infty)$. 
\section{Lecture 9}
\subsection{Series}
Consider a sequence of partial sums: $s_n = \sum{k = m}{n} a_k$. The infinite series $\sum{n = m}{\infty} a_n$ is said to \vocab{converge} if $(s_n)$ of partial sums converges to a real number $S$. Then, $\sum{n = m}{\infty} a_n = S$.

Thus, $\sum{n = m}{\infty} a_n = S$ means taht $\lim s_n = S$, or $\lim_{n \to \infty} \sum{k = m}{n} a_k = S$. If $S$ doesn't exist or is $\pm \infty$, then we say that $(s_n)$ \vocab{diverges}.

\begin{thm}
    A series converges iff it satisfies the Cauchy criterion.
    
    Recall that the Cauchy criterion is the property that for each $\epsilon > 0$, $\exists N$ such that $n \geq m > N \implies |s_n - s_{m-1}| \leq \epsilon$.
\end{thm}

\begin{proof}
    Suppose the Cauchy criterion is satisfied. Then, 
\end{proof}

\section{Lecture 10}
\subsection{Comparison Test}
Let $\Sigma a_n$ be a series where $a_n \geq 0$. Then, for a series $\Sigma b_n$:
\begin{itemize}
    \item If $|b_n| \leq a_n$ and $\Sigma a_n$ converges, then $\Sigma b_n$ converges.
    \item If $|b_n| \geq a_n$ and $\Sigma a_n$ diverges, then $\Sigma a_n$ diverges.
\end{itemize}

\begin{defn}[Absolute Convergence]
    If a series $\Sigma |a_n|$ is convergent, then so is $\Sigma a_n$. We say such a series is \vocab{absolutely convergent}.
\end{defn}

It follows that if a sequence is absolutely convergent, then it is convergent.

\subsection{Ratio Test}
A series $s_n$ of nonzero terms:
\begin{itemize}
    \item Converges absolutely if $\lim \sup |\frac{a_{n+1}}{a_n} < 1$
    \item Diverges if $\lim \inf |\frac{a_{n+1}}{a_n} > 1$ 
    \item Gives no information otherwise.
\end{itemize}

\subsection{Root Test}
Let $\Sigma a_n$ be a series and let $\alpha = \lim \sup |a_n|^\frac{1}{n}$. The series $\Sigma a_n$
\begin{itemize}
    \item Converges absolutely if $\alpha < 1$
    \item Diverges if $\alpha > 1$
    \item Gives no information otherwise.
\end{itemize}

\subsection{Integral Test}
\[\int{a}{\infty} f(x) dx = \lim_{t \to \infty} \int{a}{t} f(x) dx.\] 

\begin{thm}[$p$-series]
    $\sum{}{} \frac{1}{n^p}$ converges if and only if $p > 1$.
\end{thm}

With this, we can define the \vocab{Integral Test}. We use this when:
\begin{itemize}
    \item tests in previous sections don't work
    \item terms of $\sum{}{} a_n$ are nonnegative 
    \item decreasing function $f$ on $[1, \infty)$ such that $f(n) = a_n \forall n$.
    \item integral is easy enough to compute.
\end{itemize}

The test states:

If $\lim_{n \to \infty} \int{1}{n} f(x) dx = +\infty$, then series diverges. Else, series converges.

\begin{thm}[Alternating Series Theorem]
    If $a_1 \geq a_2 \geq \cdots \geq a_n \geq 0$, and $\lim a_n = 0$, then the alternating series $\sum{}{} (-1)^{n + 1} a_n$ converges. Moreover, partial sums $s_n$ of sequences satisfy $|s - s_n| \leq a_n$.
\end{thm}

We see that the harmonic series $a_n = \frac{1}{n}$ diverges, but the alternating harmonic series $a_n = (-1)^n \frac{1}{n}$ converges as a result.

\chapter{Week 4}
\section{Lecture 11}
\subsection{Continuity}
\begin{defn}
    Let $E \subset \RR$. A \vocab{real-valued function} is a rule $f : E \to \RR$ such that $y = f(x) \in \RR$ for all $x \in E$.

    Then, we say that $E$ is the \vocab{domain} of $f$, and $f(E) = \{f(x) : x \in E\}$ is the \vocab{range}, or the image.
\end{defn}

The epsilon-delta definition of continuity parallels the definition for limits:
\begin{defn}
    Let $f : E \to \RR$, and $x_0 \in E$. Then, $f$ is continuous at $x_0$ if, for all $\epsilon > 0$, there exists $\delta > 0$ such that $x \in E$ and $|x - x_0| < \delta$ implies $f(x) - f(x_0) < \epsilon$.
\end{defn}

Notice the new $\delta$ parameter added. We say that $\epsilon$ and $\delta$ define a \vocab{neighborhood}.

\begin{thm}
    Let $f: E \to \RR$. Then, $f$ is continuous at $x_0 \in E$ iff for all $\epsilon > 0$, there exists $\delta > 0$ such that $x \in E$ and $|x - x_0| < \epsilon \implies |f(x) - f(x_0)| < \epsilon$.
\end{thm}

\begin{thm}
    Let $f : E \to \RR$.  If $f$ is continuous at $x_0 \in E$, then $|f|, kf$ are continuous at $x_0$.
\end{thm}

\begin{thm}
    Let $f, g$ be real functions that are continuous at $x_0$ on $\RR$. Then
    \begin{enumerate}[label=(\roman*)]
        \item $f + g$ is cont. at $x_0$
        \item $fg$ is cont. at $x_0$
        \item $f/g$ is cont. at $x_0$ if $g(x_0) \neq 0$.
    \end{enumerate}
\end{thm}

\subsection{Bounded Functions}
\begin{thm}
    If a function $f : [a, b] \to \RR$ is continuous then $f$ is bounded and has a minimum and maximum.
\end{thm}

\begin{thm}[Intermediate Value Theorem]
    If $f: I \to \RR$ is continuous and $I$ is an interval, then $f$ has the intermediate value property on $I$:

    For any $a, b \in I$, $a < b$, and $y$ btwn. $f(a), f(b)$, then there exists $x \in [a, b]$ such that $f(x) = y$.
\end{thm}

\begin{proof}
    WLOG let $f(a) < y < f(b)$. 

    Let $S := \{x \in [a, b] | f(x) < y\}$. Since $a \in S$, $S \neq 0$. Moreover, $S$ is bounded above by $b$. So, there is an $x_0 \in [a, b]$ such taht $x_0 = \sup S$.

    We claim that $f(x_0) = y$.

    For all $n \in \NN$, there is an $x_n \in S$ such that $x_0 - \frac{1}{n} < x_n \leq x_0$. Thus, $\lim x_n = x_0$. Now, since $f(x_n) < y$ for all $n \in \NN$, we see that $f(x_0) = \lim_{n \to \infty} f(x_n) \leq y$. Therefore, $f(x_0) \leq y$. 

    Now, suppose we have a sequence $t_n = \min(b, x_0 + \frac{1}{n})$. Since $x_0 \leq t_n \leq x_0 + \frac{1}{n} \implies \lim t_n = x_0$. Each $t_n \in [a, b]$ but not in $S$, so $f(t_n) \geq y$. Thus, $f(x_0) = \lim f(t_n) \geq y$.

    As a result, we have $f(x_0) \leq y$ and $f(x_0) \geq y$, implying that $f(x_0) = y$.
\end{proof}

\begin{corollary}
    If $I$ is an interval or single point, then $f(I)$ is also an interval or single point for continuous $f$.
\end{corollary}

We can use this corollary to find roots.

\begin{thm}
    Let $I \subset \RR$ be an interval and $f: I \to \RR$ be monotonic. If $f(I)$ is also an interval, then $f$ is continuous.
\end{thm}
\chapter{Week 5}
\section{Lecture 14}
\subsection{Uniform Continuity}
Recall that $f(x)$ is continuous at $x_0$ if for $\epsilon > 0$, there exists $\delta (x_0, \epsilon)$ such that $|x - x_0| < \delta \implies |f(x) - f(x_0)| < \epsilon$. Note here that $f$ depends on $\epsilon > 0$, and on $x_0$ in $E$. In this section, we find a case where $\delta$ can be chosen to only depend on $\epsilon$.

\begin{defn}
    We say $f: E \to \RR$ is \vocab{uniformly continuous} if $\forall \epsilon > 0$ there exists a $\delta = \delta(\epsilon)$, s.t. for all $x, y \in E$, $|x - y| < \delta \implies f(x) - f(y) < \epsilon$.
\end{defn}

\begin{lemma}
    $P ; E \to \RR$ is not uniformly continuous on $E$ iff there exists $\epsilon > 0$ such that $\forall n \in \NN$, there exists $x_n, y_n \in E$ such that $|x_n - y_n| < \frac{1}{n}$ but $|f(x_n) - f(y_n)| \geq \epsilon$.
\end{lemma}

\begin{proof}
    Note that if $f$ is not uniformly continuous, then $|x_0 - y_0| < \frac{1}{n}$, then $|f(x_0) - f(y_0)| \geq \epsilon$.

    Suppose $\epsilon > 0$ exists such that for all $n \in \NN$, there exists $x_n, y_n \in E$ such that $|x_n - y_n| \leq \frac{1}{n}$, but $|f(x_n) - f(y_n)| \geq \epsilon$ and $f$ is uniformly continuous. Then, $\exists \delta$ such that $|x - y| < \delta \implies |f(x) - f(y)| < \epsilon$. From the Archimedean Property, there exists $n \in \NN$ such that $\frac{1}{n} < \delta$ and $x_n, y_n$ as above.

    Then, $|x_n - y_n| < \frac{1}{n} < \delta$, but $|f(x_n) - f(y_n)| \geq \epsilon$. This contradicts our initial claim that $f$ is uniformly continuous.
\end{proof}

\begin{thm}
    Let $f$ be continuous on a closed interval $[a, b]$, then it is uniformly continuous on $[a, b]$. 
\end{thm}

\begin{proof}
    Let $(s_n)$ be a Cauchy sequence in $S$ and let $\epsilon > 0$ such that $\forall x, y \in S$,
    \begin{align}
        |x - y| < \delta \implies |f(x) - f(y)| < \epsilon.
    \end{align}
    Now, since $(s_n)$ is a Cauchy sequence, $\exists N$ such that $m, n > N \implies |s_n - s_m| < \delta$ for some $\delta$. By (1), we see that $m, n > N$ implies $f(s_n) - f(s_m) < \epsilon$, that is, $(f(s_n))$ is Cauchy.
\end{proof}

\begin{ex}
    $f : (0, 1) \to \RR, f(x) = \frac{1}{x^2}$.
\end{ex}

We see that $f$ is not uniformly continuous because the domain isn't closed.
\section{Lecture 15}
\subsection{Limits of Functions}
\subsection{Basic Properties of Derivative}
\section{Lecture 16}
\begin{thm}
    Let $f : [a, b] \to \RR$. If $f$ has a local max or local min at a point $x \in (a, b)$, and if $f'(x)$ exists, then $f'(x) = 0$.
\end{thm}

\begin{proof}
    Assume $x_0$ is a local maximum. A similar argument would work for local minimum.

    Then, there exists $\delta > 0$ such that $f(x_0) \geq f(x)$, such that $a < x_0 - \delta < x < x_0 + \delta < b$.

    Then, it follows that $\frac{f(x) - f(x_0)}{x - x_0}$ is $\geq 0$ when $x < x_0$ and is $\leq 0$ when $x > x_0$. This means that as $x \to x_0$, $f'(x^+) \geq 0$ and $f'(x^-) \leq 0$. However, these two valeus must be equal because $f'(x)$. Thus, the only possible value for $f'(x)$ is $0$.
\end{proof}

\begin{thm}[Mean Value Theorem]
    Let $f$ be continuous on $[a, b]$ and differentiable on $(a, b)$.

    Then, $\exists c \in (a, b)$ such that $f'(c) = \frac{f(b) - f(a)}{b - a}$.
\end{thm}

\begin{proof}
    Consider $h(x) = f(x) - \frac{f(b) - f(a)}{b - a} \cdot x$.

    Then, we have three conditions: $h(x)$ is continuous over $[a, b]$ and differentiable over $(a, b)$, and $h(a) = h(b)$.

    We can then use Rolle's Theorem, outlined below, to finish.
\end{proof}

\begin{thm}[Cauchy's MVT]
    Assume $f, g : [a, b] \to \RR$ are continuous, and differentiable on $(a, b)$. Then, there exists $x_0$ in $(a, b)$, such that \[g'(x_0)(f(b) - f(a)) = f'(x_0)(g(b) - g(a)).\]
\end{thm}

\begin{corollary} 
    $f$ is constant on $(a, b)$ if $f'(x) = 0$ for all $x \in (a, b)$.
\end{corollary}

This is proved directly by the Mean Value Theorem.

\begin{corollary}
    Let $f$ and $g$ be differentiable functions on $(a, b)$ such that $f' = g'$ on $(a, b)$. 

    Then, there exists $C$ such that $f(x) = g(x) + C$ for all $x \in (a, b)$.
\end{corollary}
\section{Lecture 17}
\subsection{Riemann Integral}
\begin{defn}
    Let $[a, b]$ be a given interval. By a \vocab{partition} $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1, \ldots, x_n$, where $a = x_0 \leq x_1 \leq \ldots \leq x_{n - 1} \leq x_n = b$.

    For any $i = 1, \ldots, n$, we define $\Delta x_i = x_i - x_{i - 1}$. 
\end{defn}

Then, we take Riemann sums.

Suppose $f : [a, b] \to \RR$ is bounded. Let $P$ be a partition of $[a, b]$. We put $M_i = \sup f(x), x_{i-1} \leq x \leq x_i$, and $m_i = inf f(x) x_{i-1} \leq x \leq x_i$. 

Then, $M_i$ gets too much area and $m_i$ loses area. That is $U(P, f) = \sum{i = 1}{n} M_i \Delta x_i$, and $L(P, f) = \sum{i = 1}{n} m_i \Delta x_i$. 

The upper Riemann integrals of $f$ are $\int{a}{\overline b} f dx = \inf U(P, f)$. The lower Riemann integrals of $f$ are $\int{\underline a}{b} f dx = \sup L(P, f)$.

There must exist $m, M \in \RR$ such that $m \leq f(x) \leq M$. Thus, $m(b - a) \leq f(x)(b - a) \leq M(b - a)$. Thus, for every $P$, $m(b - a) \leq \sum{i = 1}{n} m_i \Delta x \leq \sum{i = 1}{n} M_i \Delta x \leq M(b - a)$.

If lower and upper integrals are equal, then we just have integral. Another name for lower/upper integrals are Darboux integrals.

\begin{defn}
    Given a partition $P$, a partition $P^*$ is called a \vocab{refinement} of $P$ if $P \subseteq P^*$ (that is, if every point of $P$ is a point of $P^*$). 
\end{defn}

Given two partitions $P_1, P_2$, we call $P*$ the \vocab{common refinement} if $P^* = P_1 \cup P_2$.

\begin{lemma}
    If $P \subseteq P^*$, then $L(P, f) \leq L(P^*, f) \leq U(P^*, f) \leq U(P, f)$.
\end{lemma}

\subsection{Integrability}
\begin{thm}
    Let $f: [a, b] \to \RR$ be bounded and monotonic. Then, $f \in R[a, b]$.

    This means $f$ is a Riemann integrable function. This means it must have upper and lower Riemann integrals the same.
\end{thm}

\begin{thm}
    $f \in R([a, b])$ iff for all $\epsilon > 0$, there exists a partition $P$ such that $U(P, f) - L(P, f) < \epsilon$.
\end{thm}

\begin{thm}
    \[\int{\underline{a}}{b} f(x) \leq \int{a}{\overline{b}} f(x).\]
\end{thm}
\chapter{Week 6}
\section{Lecture 18 - Metric Spaces}
\begin{defn} Let $X$ be a set. A \vocab{metric} (distance function) $d$ on $X$ is a function $d : X \times X \to \RR, (x, y) \to d(x, y)$.
\end{defn}

\begin{defn}[Metric Space]
    A metric space is comprised of a set $X$, distance function $d : X \times X \to \RR$.

    We have multiple properties to make $(X, d)$ a metric space:
    \begin{enumerate}
        \item \textbf{Positivity}. $d(x, y) > 0$ if $x \neq y$, $x, y \in X$ or $d(x, x) = 0$.
        \item \textbf{Symmetry}. $d(x, y) = d(y, x)$, for all $x, y \in X$.
        \item \textbf{Triangle Inequality}.
    \end{enumerate}
\end{defn}

\begin{ex}
    $(\RR, d)$ is a metric space with $d(x, y) := |x - y|$. 
\end{ex}

Clearly, $d$ is a metric on $\RR$, as $d : \RR \times \RR \to \RR$. This satisfies:
\begin{enumerate}
    \item $d(x, y) = |x - y| > 0, d(x, x) = |x - x| = 0$ for all $x \neq y, x, y \in \RR$.
    \item $d(y, x) = |y - x| = |-(x - y)| = |x - y| = d(x, y)$ for all $x, y \in \RR$.
    \item $d(x, y) = |x - y| = |x - z + z - y| \leq |x - z| + |z - y| = d(x, z) + d(z, y)$ for all $x, y, z \in \RR$.
\end{enumerate}

\begin{defn}[Euclidean Space]
    For $n \geq 1$, define $n$-dimensional Euclidean space:
    \[\RR^n = \{\vec x = (x_1, \ldots, x_n) | x_j \in \RR, 1 \leq j \leq n \}.\] 
\end{defn}

\begin{ex}[Discrete Metric]
    Let $x$ be any set. Define $d_d(x, y) = \begin{cases} 1 & x \neq y \\ 0 & x = y \end{cases}$.
\end{ex}
\begin{enumerate}
    \item Positivity holds by definition.
    \item $d_d (x, y) = d_d (y, x) = 1$ for $x \neq y$.
    \item $d_d (x, y) \leq d_d (x, z) + d_d (z, y)$
\end{enumerate}

\begin{defn}[balls]
    Let $(X, d)$ be a metric space.
    
    For any $x \in X$, $r > 0$,
    \begin{enumerate}
        \item The subset $B_r(x) := \{y \in X | d(x, y) < r\}$ is called the \vocab{open ball} centered at $x$ with radius $r$.
        \item The subset $\overline{B_r(x)} := \{y \in X | d(x, y) \leq r\}$ is called the \vocab{closed ball} centered at $x$ with radius $r$.
        \item An open ball centered at $x$ is also a \vocab{neighborhood} of $x$.
    \end{enumerate}
\end{defn}

In $\RR$, each open/closed ball is equivalent to a finite open/closed interval.

\begin{defn}[Openness]
    Let $(X, d)$ be a metric space. A subset $A \subset X$ is called \vocab{open} if $A = \phi$ or if for every $x \in A$, there exists some open ball $B_r(x) \subset A$ for some $r > 0$.
\end{defn}

\begin{thm}
    Any open ball is open.
\end{thm}

\section{Lecture 19 - Also metric spaces}
\subsection{Types of Points in Set}
\begin{defn}[Interior Point]
    Let $E \subset X$. A point $x \in X$ is said to be an \vocab{interior point} of $E$ if it has a neighborhood $U$ lying entirely enside $E$, that is, $U \subset E$.
\end{defn}

In other words, an interior point is a point $x \in E$ if $\exists r = r(x) > 0$ such that $B_r(x) \subset E$. The set of interior points is called the \vocab{interior} of $E$, or $E^\circ = \{ X \in E | \exists r = r(x) > 0, B_r(x) \subset E\}$.

\begin{ex}
    $E = [-3, 3] \ \{0\} \implies E^\circ = (-3, 0) \cup (0, 3)$. 
\end{ex}

\begin{thm}
    $E$ open $\iff$ $E^\circ = E$.

    Also, $E^\circ$ is the largest open set in $E$.
\end{thm}

\begin{proof}
    Suppose $E$ is open. Then, any $x \in E$ must be an interior point by definition of openness (there must be some open ball $B_r(x) \subset E$ for each $x$). As such, $E \subseteq E^\circ$. But we know that $E^\circ \subseteq E$, so $E^\circ$ must be open.

    Suppose that $U \subset E$ and $U$ is an open subset of $X$. Then, if $x \in U$, there exists an $r > 0$ such that $B_r(x) \subset U$ but $U \subset E \implies B_r(x) \subset E$. Therefore, $x \in E^\circ$, so $U \subset E^\circ$. 
\end{proof}

\begin{thm}
    Assume $(X,d)$ is a metric space.
    \begin{enumerate}
        \item Both $\emptyset$ and $X$ are open.
        \item If $\{E_\alpha\}_{\alpha \in \Lambda}$ is a collection of open sets, then $\bigcup_{\alpha\in \Lambda} E_\alpha$ is open.
        \item If $E_1, E_2, \ldots, E_N$ is a finite collection of open sets, then $\bigcap_{i = 1}^N E_i$ is open.
    \end{enumerate}
\end{thm}

\begin{remark}
    The above theorem is not true for arbitrary intersections. Consider $I_n = (-\frac{1}{n}, -\frac{1}{n}) \subset (\RR, 1 \cdot 1)$ open but $\bigcap_{i=1}^\infty I_i$ is not open as it is the null set.
\end{remark}

\begin{defn}[Limit Points]
    Let $E \subset (X, d)$. A point $x \in X$ is called a \vocab{limit point} of $E$ if $\forall r > 0$, $B_r(x)$ intersects $E$ at some point other than $x$, i.e. $x$ is a limit point of $E$ if for any neighborhood $B_r(x)$, there exists a $y \in B_r(x) \cap E$ s.t. $y \neq x$.
\end{defn}

We use $E'$ to determine all of the limit points of $E$, and use $\overline{E} = E \cup E'$ to denote the intersection of $E$ with its limit point set, calling it the \vocab{closure} of $E$ in $X$. Points in $E \setminus E'$ are called \vocab{isolated}. 

We have a bunch of definitions coming up, so let $(X, d)$ be a metric space and $E \subset X$. 

\begin{defn}[Complement]
    The complement $E^C$ of $E$ is the set of all points $x \in X$ such that $x \notin E$. 
\end{defn}

\begin{defn}[Closed]
    $E \subset X$ is called \vocab{closed} if every limit point of $E$ is a point of $E$, that is, $E' \subset E$. 
\end{defn}

\begin{defn}[Perfect]
    $E$ is \vocab{perfect} if $E$ is closed and if every point of $E$ is a limit point.
\end{defn}

\begin{defn}
    $E$ is \vocab{bounded} if there is a real number $M$ and a point $y \in X$ such that $d(x, y) < M \forall X \in E$. 
\end{defn}

\begin{thm}
    A set $E$ is open iff its complement is closed.
\end{thm}

\begin{corollary}
    A set $F$ is closed iff its complement is open.
\end{corollary}

\begin{thm} 
    Assume $(X, d)$ is a metric space. 
    \begin{enumerate}
        \item Both $X$ and $\emptyset$ are closed.
        \item For any finite collection $F_1, \ldots, F_n$ of closed sets, $\bigcup_{i = 1}^n F_i$ is closed.
        \item For any collection $\{F_\alpha\}$ of closed sets, $\bigcap_{\alpha} F_\alpha$ is closed.
    \end{enumerate}
\end{thm}

\begin{thm}
    If $X$ is a metric space and $E = X$, then
    \begin{enumerate}
        \item $\overline E$ is closed.
        \item $E = \overline E$ iff $E$ is closed.
        \item $\overline E \subset F$ for every closed set $F \subset X$ s.t. $E \subset F$.
    \end{enumerate}
\end{thm}

\section{Lecture 20}
\begin{thm}
    If $X$ is a metric space and $E \subset X$:
    \begin{enumerate}
        \item $\overline E$ is closed.
        \item $E = \overline E \iff$ $E$ is closed.
        \item $\overline E \subset F$ for every closed set $F \subset X$ s.t. $\overline E \subset F$.
    \end{enumerate}
\end{thm}

\begin{proof}
    If $E = X$, then we see that it must be closed by definition. If not, then for any $X \in \overline{E}^C$, we show that there must be some open ball $B_r(x) \subset \overline{E}^C$. 

    Suppose this is false. Then, for 
\end{proof}

\begin{thm}
    Let $E$ be a nonempty set of real numbers which is bounded above. Let $Y = \sup E$. Then, $y \in \overline E$. Hence, $Y \in E$ if $E$ is closed.
\end{thm}

\begin{thm}[Bolzano-Weierstrass]
    Every bounded sequence in $\RR^k$ has a convergent subsequence.
\end{thm}

\section{Lecture 21}
\subsection{Compactness}
First, we need to define a couple of terms before we define compactness. A \vocab{family} of sets is a collection of subsets of $X$. 

\begin{defn}[Open Cover]
    An \vocab{open cover} of $E \in (X, d)$, which we will call $\{ G_\alpha \}$, is a family of sets such that $E \subset \bigcup_\alpha G_\alpha$.

    In other words, each point in $E$ must also be in one of the sets $G_\alpha$.
\end{defn}

\begin{defn}[Compact]
    A set $E$ is considered \vocab{compact} if any open cover of $E$ has a finite subcover.
\end{defn}

\begin{thm}[Heine-Borel]
    A set $E$ is compact if and only if it is closed and bounded.
\end{thm}

\begin{proof}
    Suppose $E$ is compact. Then, for each $m \in \NN$, let $U_m$ consist of all $x$ in $\RR^k$ such that $\max \{|x_j| : j = 1,2,3,\ldots, k\} < m$.

    The family $\mathcal{U} = \{U_m : m \in N\}$ is an open cover of $E$, so a finite subfamily of $\mathcal{U}$ covers $E$. If $U_{m_0}$ is the largest member of the subfamily, then we see that $E \subseteq U_{m_0}$. Thus, $E$ must be bounded, as each $U_m$ is bounded for finite $m$.

    To show that $E$ is closed, we prove that its complement is open. Consider a point $x_0 \in E^C$. For $m \in \NN$, let $V_m = \{x \in \RR^k : d(x, x_0) > \frac{1}{m}\}$. Then, each $V_m$ is open in $\RR^k$, and $\mathcal{V} = \{V_m\}$ covers $E$ since the union of all $V_m$ is $\RR^k \setminus \{x_0\}$. Since $E$ can be covered by finitely many $V_m$, we see that $E \subset \{X \in \RR^k: d(x, x_0) > \frac{1}{m_0}$. Then, $\{X \in \RR^k: d(x, x_0) < \frac{1}{m_0}\} \subset E^C$. Thus, $x_0$ must be interior $E^C$ for any $x_0 \in E^C$, so $E^C$ must be open and $E$ must be closed. \qed

    Now, suppose $E$ is closed and bounded. Since $E$ is bounded, $E$ is the subset of a set having the form $F = \{x \in \RR^k : |x_j| \leq m \forall j \in [1, k]\}$. 

    Note that $F$ is a $k$-cell, or a rectangle in $k$ dimensions. We define the \vocab{diameter} of a $k$-cell to be \[\sup \{d(x, y) \forall x, y \in F\}.\] Now, we have the following lemma:

    \begin{lemma}
        Every $k$-cell $F$ in $\RR^k$ is compact.
    \end{lemma}

    \begin{proof}
        We assume for the sake of contradiction that $F$ is not compact. Then, there must be some open cover $\mathcal{U}$ of $F$ such that there is no finite subcover that covers $F$.

        Let the diameter of $F$ be $\delta$. We see that if $F$ cannot be covered by a finite subcover, then it must contain a $k$-cell $F_1$ of diameter $\frac \delta 2$ such that $F_1$ is also not able to be contained by any finite subcover.

        Repeating this logic infinitely, we have that $F \supset F_1 \supset F_2 \supset F_3 \supset \cdots$, and no $F_n$ is able to be fit into a finite subcover. However, we see that $\bigcap_{i = 1}^\infty F_i = x_0$ for some $x_0$. This contradicts the claim we initially made because $x_0$ can fit into a finite subcover by definition.
    \end{proof}

    This lemma concludes our proof.
\end{proof}

\chapter{Week 7}
\section{Lecture 22}
\begin{hw}
    Let $X$ be the set of continuous functions from $[a, b]$ to $\RR$. For all $x, y \in X$ let $d(x, y)$ be \[d(x, y) = \max \{ |x(t) - y(t) \}.\]

    Show that $(X, d)$ is a metric space.
\end{hw}

\begin{proof}
    We only need to confirm that $d$ is a valid metric in $X$.
\end{proof}

\begin{hw}
    Let $(X, d)$ be a metric space. Suppoes $\rho$ is defined as \[\rho(x, y) = \frac{d(x, y)}{1 + d(x, y)}.\] 

    Show that $\rho$ is a metric on $X$. (Note that the new metric $\rho$ is bounded because $\rho(x, y) < 1$ for all $x, y \in X$.)
\end{hw}

\begin{proof}
    Once again, we only need to prove that this is a metric.

    \textbf{Positivity.} Note that $d(x, y) \geq 0$, so for any nonnegative $d(x, y)$, we find $\rho(x, y)$ to have a nonnegative numerator and positive denominator. Furthermore, $\rho(x, y)$ only equals $0$ when $d(x, y) = 0$, or $x = y$. Therefore, positivity must be satisfied.

    \textbf{Commutativity.} $\rho(y, x) = \frac{d(y, x)}{1 + d(y, x)} = \frac{d(x, y)}{1 + d(x, y)} = \rho(x, y)$.

    \textbf{Triangle Inequality.} $\rho(x, y) + \rho(y, z) = \frac{d(x, y)}{1 + d(x, y)} + \frac{d(y, z)}{1 + d(y, z)}$.
\end{proof}

\begin{hw}
    Let $A$ be a subset of $(M, D)$. Prove that 
    \begin{enumerate}
        \item $A = \Cap\{C : A \subset C, C \text{ closed}\}$.
        \item $A - \overline A$ if and only if $A$ is closed.
    \end{enumerate}
\end{hw}

\subsection{Back to Compactness}
\begin{defn}
    Assume $(X, d)$ is a metric space. A subset $K \in X$ is called \vocab{sequentially compact}, if every sequence of points in $K$ has a convergent subsequence converging to a point in $K$.
\end{defn}

It follows that if a subset $K$ is compact, then it is sequentially compact.

\subsection{Continuity and Limits}
Let $(X, d_X), (Y, d_Y)$ be metric spaces. 

\begin{defn}
    Let $E \subset X$ and $f: E \to Y$.
    \begin{enumerate}
        \item If $x_0 \in E'$, we say $\lim_{x \to x_0} f(x) = y_0$ if $\forall \epsilon > 0 \exists \delta > 0$ such that \[d_X(x, x_0) < \delta \implies d_y(f(x), y_0) < \epsilon,\] for any $x \in E \setminus \{x_0\}$.
        \item $f$ is said to be \vocab{continuous} at $x_0 \in E$ if $\lim_{x \to x_0} f(x) = f(x_0) \iff \forall \epsilon > 0 \exists \delta > 0$ such that \[d_X(x, x_0) < \delta \implies d_Y(f(x), f(x_0)) < \epsilon.\] 
    \end{enumerate}
\end{defn}

\section{Lecture 23}
\subsection{Continuity and Limits, continued}
\begin{definition}[Pointwise Convergence]
    A sequence of functions $f_n$ is \vocab{pointwise convergent} to a particular function if $\lim_{n \to \infty} f_n(x) = f(x)$ for every $x$ in the domain of $f$.
\end{definition}

However, pointwise convergence is weak in a number of scenarios, and uniform convergence provides a stronger definition of convergence.

\begin{definition}[Uniform Convergence] 
    A sequence of functions $f_n$ is \vocab{uniformly convergent} on $S$ to a function $f$ if for each $\epsilon > 0$ there exists a number $N$ such that $|f_n(x) - f(x)| < \epsilon$ for all $X \in S$ and all $n > N$.
\end{definition}

Consider the example $f_n(x) = x^n$ over the domain $[0, 1]$. 

We see that $\lim_{n \to \infty} f_n(x) = 0$ for all $x \in [0, 1)$ and $1$ for $x = 1$. Therefore $(f_n)$ is pointwise convergent to $f(x) = \begin{cases} 0 & x \in [0, 1) \\ 1 & x = 1 \end{cases}$.

Each $x^n$ is continuous, so even for large $n$ there must be $x$ that are infinitesimally close to $1$. However, for uniform convergence to $f(x)$, we need all values that are not $1$ to converge to $0$. As this is impossible for any function $x^n$, this sequence cannot be universally convergent.

The professor says to compare pointwise and uniform convergence to getting in A+ in one versus all classes.

\begin{thm}
    The uniform limit of continuous functions is continuous.
\end{thm}

\section{Lecture 24}

\begin{remark}
    If $f_n$ uniformly converges to $f$, then in particular $f_n$ pointwise converges to $f$. 
\end{remark}

\begin{thm}
    Let $E \subseteq R$ and $f_n: E \to \RR$. $f_n$ is uniformly convergent to $f$ if and only if for all $\epsilon > 0$, there exists $N$ such that $m, n \geq N$ implies $|f_n(x) - f_m(x)| < \epsilon$.
\end{thm}

\begin{proof}
    Suppose $f_n$ is uniformly convergent to $f$ on $E$. Then $\exists N$ such that $n \geq N, x \in E$ implies that $|f_n(x) - f(x)| < \frac{\epsilon}{2}$, so that 
    $|f_n(x) - f_m(x)| \leq |f_n(x) - f(x)| + |f(x) - f_m(x)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} < \epsilon$.

    We don't need to use $N_1, N_2$ because we are using the same sequence.

    Conversely, let $x \in E$. Since $(f_n)$ is uniformly Cauchy, in particular $(f_n(x))$ is a Cauchy set. So $\lim_{n \to \infty} f_n(x)$ exists. Let $f(x) = \lim_{n \to \infty} f_n(x)$, i.e., $f_n \to f$ pointwise.

    We claim that $f_n$ is uniformly convergent to $f$ on $E$. Let $\epsilon > 0$. There exists $N$ such that $n, m > N$ and $x \in E$ implies $|f_n(x) - f_m(x)| < \frac{\epsilon}{2}$. Let $m \to \infty$. Since absolute value is continuous, it follows that $|f_n(x) - f(x)| = \lim_{m \to \infty} |f_n(x) - f_m(x)| \leq \frac{\epsilon}{2} < \epsilon$.

    Therefore, $f_n$ is uniformly convergent to $f$.
\end{proof}

In other words, we now have that the Cauchy property implies uniform convergent.

\begin{thm}
    Suppose $f_n(x) \to f(x)$, $x \in E$. Let $M_n = \sup_{x \in E} |f_n(x) - f(x)|$. Then, $f_n$ is uniformly convergent to $f$ on $E$ if and only if $M_n \to 0$ as $n \to \infty$.
\end{thm}

\begin{proof}
    Suppose $(f_n)$ is uniformly convergent to $f$. Let $\epsilon > 0$, $\exists N$ such that $\forall n > N, x \in E$ $|f_n(x) - f(x)| < \epsilon$. Then, $\forall n > N, 0 \leq M_n = \sup |f_n(x) - f(x)| \leq \epsilon$. Thus, $0 \leq |M_n| \leq \epsilon$ for all $n > N$, so $M_n \to 0$.

    Conversely, let $\epsilon > 0$. Then, there exists $N$ such that $n > N \implies M_n < \frac{\epsilon}{2}$, that is, $\forall x \in E$ $|f_n(x) - f(x)| < \frac{\epsilon}{2}$.
    
    Now, let $n, m > 0$
\end{proof}

\begin{thm} 
    Let $f_n : E \to \RR$ be continuous at $x \in E$. If $f_n$ uniformly converges to $f$ on $E$, then $f$ is continuous at $x$.
\end{thm}

We can generalize this theorem for the following statement on interchanging limits:

Let $f_n : E \to \RR$ and $f_n$ uniformly converges to $f$ on $E$. Suppose $x_0$ is a limit point of $E$ and $\lim_{x \to x_0} f_n(x) = A_n$. Then, $(A_n)$ converges, $\lim_{x \to x_0} f(x)$ exists.

$\lim_{x \to x_0} f(x) = \lim_{x \to x_0} \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \lim_{x \to x_0} f_n(x) = \lim_{n \to \infty} A_n$.

\section{Lecture 25}
\subsection{Integration}
\begin{thm}
    Let $f_n \in R[a, b]$ and $f_n$ uniformly converges to $f$ on $[a, b]$. Then, $f \in R[a, b]$. Moreover, \[\int{a}{b} f(t)dt = \lim_{n \to \infty} \int{a}{b} f_n(t)dt.\] 
\end{thm}

\subsection{Differentiation}
\begin{thm}[Rudin 7.17]
    Suppose $(f_n)$ is a set of differentiable functions on $[a, b]$ such that 
    \begin{enumerate}
        \item $f_n \to f$ pointwise on $[a, b]$. 
        \item $f'_n \to g$ uniformly on $[a, b]$.
    \end{enumerate}

    Then, $f_n \to f$ uniformly on $[a, b]$, and $f$ is differentiable on $[a, b]$. Moreover $f'(x) = g(x)$ for all $x \in [a, b]$, that is, $f'(x) = \lim_{n \to \infty} f_n'(x) \forall a \leq x \leq b$.
\end{thm}

\begin{proof}
    Let $\epsilon > 0$. Choose $N$ such that $n \geq N, m \geq N$ implies \[|f_n(x_0) - f_m(x_0)| \leq \frac{\epsilon}{2}.\] As $f_n'$ converges uniformly, \[f_n'(t) - f_m'(t)| < \frac{\epsilon}{2(b-a)} (a \leq t \leq b).\] 

    Apply the Mean Value Theorem to the function $f_n - f_m$, and we get that \[|f_n(x) - f_m(x) - f_n(t) + f_m(t)| \leq \frac{|x = t| \epsilon}{2(b - a)} \leq \frac \epsilon 2.\] 

    Then, the inequality $|f_n(x) - f_m(x)| \leq |f_n(x) - f_m(x) - f_n(x_0) + f_m(x_0)| + |f_n(x_0) - f_m(x_0)|$ implies that $|f_n(x) - f_m(x)| < \epsilon$, for $x \in [a, b], n, m \geq N$. Thus, $(f_n)$ converges uniformly on $[a, b]$. \qed

    Let $f(x) = \lim f_N(x) (a \leq x \leq b)$. Fix a point $x$ on $[a, b]$ and define $\phi_n(t) = \frac{f_n(t) - f_n(x)}{t - x}, \phi(t) = \frac{f(t) - f(x)}{t - x}$. Then, $\lim_{t \to x} \phi_n(t) = f_n'(x)$ for all $n \in N$. 

    Then, we see that \[|\phi_n(t) - \phi_m(t)| \leq \frac{\epsilon}{2(b - a)} (n, m \geq N),\] so that $(\phi_n)$ converges uniformly, for $t \neq x$. Since $(f_n)$ converges to $f$, we conclude that $\lim_{n \to \infty} \phi_n(x) = \phi(x)$.

    Swapping limits, we see that $\lim_{t \to x} \phi(t) = \lim_{n = \infty} f_n'(x)$.
\end{proof}
\end{document}
\documentclass{article}
\usepackage{michael}

\setlength{\parindent}{0pt}
\pagestyle{fancy}
\lhead{Linear Programming}
\rhead{Albert Ye}

\title{Linear Programming}
\author{Albert Ye}
\date{\today}

\begin{document}
\maketitle
\section{Duality}
We want to find $\max 5x_1 + 4x_2$ such that 
\begin{align*}
    2x_1 + x_2 &\leq 100 (\cdot y_1) \\
    \phantom{2}x_1 \phantom{+ x_2} &\leq 30 (\cdot y_2) \\
    \phantom{2x_1 +} x_2 &\leq 60 (\cdot y_1)
\end{align*}

Then, we have that $(2y_1+y_2) \cdot x_1 + (y_1 + y_3) \cdot x_2 \leq 100y_1 + 30y_2 + 60y_3$.

Now, we want to find $\min 100y_1 + 30y_2 + 60y_3$ such that
\begin{align*}
    y_1, y_2, y_3 &\geq 0 \\
    5 &\leq 2y_1 + y_2 \\
    4 &\leq y_1 + y_2
\end{align*}

This results in another linear program: the \vocab{dual} linear program. We then have $5x_1 + 4x_2 \leq 100y_1 + 30y_2 + 60y_3$.

In matrix form, we have:

\textbf{Primal LP:}
$\max C^T \vec{x}$ s.t. $A \vec x \leq \vec b$ and $\vec x \geq 0$.

\textbf{Dual LP:}
$\min \vec b^T \vec y$ s.t. $A^t \vec y \geq c$ and $\vec y \geq 0$.

\subsection{Weak and Strong Duality}

\begin{theorem}[Weak Duality]
    The value of the feasible solution $\vec x$ to primal linear program \textbf{must be} $\leq$ the value of the feasible solution $\vec y$ to dual linear program.
\end{theorem}

\begin{proof}
    FILL IN LATER
\end{proof}

\begin{theorem}[Strong Duality]
    If the primal opt is bounded, then primal opt = dual opt.
\end{theorem}

For example, min cut = max flow.

\subsection{Zero-sum Games}

\textbf{Input:} a payoff matrix $M$, with the row and column values determining different actions. The row player picks row $r$ and column player picks row $c$. 

The row player wins $+M_{rc}$ and the column player wins $-M_{rc}$. It's called a \vocab{zero-sum game} because the scores sum to $0$.

\textbf{Types of Strategies:}
\begin{enumerate}
    \item \vocab{Pure Strategy}: a single row / column, e.g. the row player always picks rock (beaten by column player always playing paper)
    \item \vocab{Mixed Strategy}: probability distribution over pure strategies. For example, $\Pr[\mathrm{Rock}] = \frac{1}{3}, \Pr[\mathrm{Paper}] = \frac{1}{3}, \Pr[\mathrm{Scissors}] = \frac{1}{3}$. The average score is $0$ regardless of what the column player does.
\end{enumerate}

\begin{ex}[Game 1]
    \[M = \begin{bmatrix}3 & -1 \\ -2 & 1 \end{bmatrix}.\] Row player goes first, column player goes second.
\end{ex}

First, the row player announces a mixed strat $p = (p_1, p_2)$, then the column player announces a mixed strat $q = (q_1, q_2)$.

\begin{defn}[Average Score]
    Row player's \vocab{average score} is $\mathrm{Score}(p, q) = 3p_1q_1 - 1p_1q_2 -2p_2q_1 + 1p_2q_2$.
\end{defn}

The column player's best strat is to minimize $\mathrm{Score}(p, q)$ over all mixed strategies $q$, which is equivalent to minimizing $\mathrm{Score}(p, q)$ over all pure strategies $q$. This is equal to $\min(3p_1 - 2p_2, -p_1 + p_2)$, which is the minimum of the column 1 and column 2 score.

The row player will have to pick $(p_1, p_2)$ that gives them the maximum $\mathrm{Score}(p, q)$. We can compute the $\max_p(\min(3p_1 - 2p_2, -p_1 + p_2))$ with linear programming.

\begin{proof}
    Maximize $z$, subject to 
    \begin{align*}
        z &\leq 3p_1 - 2p_2 \\
        z &\leq -p_1 + p_2 \\
        p_1 + p_2 &= 1 \\
        p_1 \geq 0&, p_2 \geq 0.
    \end{align*}
    This is a linear program. Note that $z = \min(3p_1 - 2p_2, -p_1 + p-2)$.
\end{proof}

\begin{example}[Game 2]
    Same as game $1$, except the column player goes first and the row player goes second.
\end{example}

Now, the row player does a pure strategy and the col player does a mixed strategy.

Note that the payoff of row $1$ is $3q_1 - q_2$ and the payoff of row $2$ is $-2q_1 + q_2$. So the \textbf{row} player's best strat is $\max (3q_1 - q_2, -2q_1 + q_2)$. As a result, the column player's best strat is $\min_q max(3q_1 - q_2, -2q_1 + q_2)$, where $\min_q(x)$ means the minimum value of $x$ over all strategies $q$.

Comparing the two games, we have $\max_p(\min_q(score(p, q))) \leq \min_q(\max_p(score(p, q)))$. We can go further with strong duality, though:

\begin{theorem}[Min-Max Theorem]
    $\max_p(\min_q(score(p, q))) = \min_q(\max_p(score(p, q)))$
\end{theorem}

\subsection{Experts Problem}
There are $n$ experts $E_1, \ldots, E_n$. Every day, they make a prediction. On the $t$th day, 
\begin{enumerate}
    \item Pick and expert $E_i$
    \item Each expert $E_j$ incurs a loss $l_j^{(t)} \in \{0, 1\}$. 
    \item You incur the loss $l_i^{(t)}$
\end{enumerate}

Allowed to pick a random expert on the $t$th day.

\begin{remark}
Surprising features of the model:
\begin{enumerate}
    \item \textbf{Not} assuming that past performance predicts future performance

    $\therefore$ all algorithms are useless.
    \item Expert losses can be adversarial, that is, losses can depend on the distribution you specify.
\end{enumerate}
\end{remark}

\begin{theorem}
    There exists an algorithm with regret $R^T \leq 2 \sqrt{T \ln n}$.

    $\therefore$ after time $T$, \vocab{average regret} $\frac{R^T}{T} \leq \frac{2 \sqrt{\ln n}}{\sqrt T}$, so as $T \to \infty$, $\frac{R^T}{T} \to 0$.
\end{theorem}
\end{document}

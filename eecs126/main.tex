\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage[portrait, margin=0.5in, top=0.8in, bottom=0.3in]{geometry}
\usepackage{amsmath, amssymb}
\setlength{\parskip}{1em}
\setlength{\parindent}{0cm} 
\usepackage[mdthm,simplethm]{test}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{physics}
\usepackage{sectsty}
\usepackage{tikz}
\usepackage{circuitikz}
\allsectionsfont{\normalfont\sffamily\bfseries}

\newcommand{\Var}{\mathrm{Var}}

\setlength{\parindent}{0pt}
\pagestyle{fancy}
\lhead{EECS 126}
\rhead{Albert Ye}

\title{EECS 126}
\author{albert ye}
\date{\today}

\begin{document}
\maketitle
\section{Probability Space}
\subsection{Definition}
Essentially from 70. Events happen with some probability in a larger probability space that contains all events that can happen.
\subsection{Axioms of Probability}
\begin{proposition}[Axioms]
	\begin{enumerate}
		\item (Positivity) $P(\omega > 0)$ for any event $\omega$ in probability space $\Omega$. 
		\item (Totality) In any sample space $\Omega$, $P(\Omega) = 1$.
		\item (Additivity) If $A_1, A_2, \ldots, A_n$ are independent, then \[\sum_{i=1}^n A_i = \bigcup_{i = 1}^n A_i.\] 
	\end{enumerate}
\end{proposition}

From just this, we can get some useful information, such as the union bound.
\begin{theorem}[Union Bound]
	\[P\left(\bigcup_{i = 1}^n A_i\right) = \sum_{i = 1}^n P(A_i).\]
\end{theorem}

The proof is left as an exercise to the student, probably in the homework.
\subsection{$\sigma$-algebra}
\begin{definition}[$\sigma$-algebra]
	Given a sample space $\Omega$, a set $\mathcal{F} \subseteq 2^{\Omega}$ is a $\sigma$-algebra if:
	\begin{enumerate}
		\item $\Omega \in \mathcal{F}$
		\item If any event $A$ is in $\mathcal{F}$, then its complement $\Omega \setminus A$ is also in $\mathcal{F}$.
		\item For countably many events $A_1, A_2, \ldots, A_n, \ldots \in \mathcal{F}$, their union $A = \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
	\end{enumerate}
\end{definition}

The biggest note is that $\Omega$ must be in a $\sigma$-algebra in order for any of the axioms of probability to apply.
\newpage
\section{Conditional Probability}
\subsection{Definition}
\subsection{Total Probability}
\subsection{Bayes' Rule}
\subsection{Continuous Bayes}

\newpage
\section{It Depends}
\subsection{Independence / (Un)correlation}
\subsection{Conditional Expectation}
Notice that $E[X | Y]$ is a random variable, but $E[X | Y = y]$ is a number. We can call $E[X | Y]$ a function $g(Y)$, where then $E[X | Y = y] = g(y)$ is just a value in the function.
\subsection{Iterated Expectation}

\newpage
\section{Distributions}
\subsection{Joint Distribution}
\begin{definition}[Joint Distribution]
	A joint distribution $f_{X, Y}(x, y)$ 
\end{definition}
\subsection{Marginal Distribution}
\subsection{Derived Distribution}

\newpage
\section{Random Variables}
\subsection{Discrete}
\subsubsection{Bernoulli}
\begin{itemize}
	\item PMF: $p_X(k) = \begin{cases}p & k = 1 \\ 1=p & k = 0\end{cases}$
	\item Expected value: $p$
	\item Variance: $p(1 - p)$.
\end{itemize}
\subsubsection{Binomial}
\begin{itemize}
	\item PMF: $p_X(k) = \binom{n}{k} p^k(1 - p)^{n - k}$ over all $k \in 0, 1, \ldots, n$.
	\item Expected value: $np$
	\item Variance: $np(1 - p)$.
\end{itemize}

Run a Bernoulli test $n$ times, find how many are positive.
\subsubsection{Geometric}
\begin{itemize}
	\item PMF: $p_X(k) = (1 - p)^{k - 1} p$, for $k = 1, 2, \ldots$.
	\item Expected value: $\frac{1}{p}$
	\item Variance: $\frac{1 - p}{p^2}$.
\end{itemize}

Here, each trial has a $p$ probability of success, and we want to find the \# of trials until one success.
\subsubsection{Poisson}
\begin{itemize}
	\item PMF: $p_X(k) = \frac{\lambda^k(e^{-\lambda})}{k!}$.
	\item Expected value: $\lambda$
	\item Variance: $\lambda$
\end{itemize}

Used to simulate arrivals, I guess. More useful later, with Poisson processes.
\subsection{Continuous}
\subsubsection{Uniform}
\subsubsection{Exponential}
\subsubsection{Gaussian}
\subsubsection{Joint Gaussian}
The main tips for Joint Gaussian are to approach it as a sort of vectorized Gaussians over a certain number $N$ of dimensions. 
Most of the addition / whatever operations in a Gaussian can be remodeled as a Joint Gaussian.

\section{Moment Generating Functions}
\begin{definition}
    The \vocab{moment generating function} (also known as a transform) associated with a RV $X$, is a function $M_X(s)$ of a scalar parameter $s$ defined by $M_X(s) = E(e^{sX})$.
\end{definition}

the simpler notation $M(S)$ can be used whenever the underlying random variable $X$ is clear from context. In more detail, when $X$ is a discrete random variable, the corresponding MGF is given by \[M(s) = \sum_{x} e^{sx} p_X(x).\] Analogously, when continuous, we just replace the summation with an integral to get \[M(s) = \int_{-\infty}^{\infty} e^{sx} f_X(x) dx.\]

Just an example so that I know what the reference is here:
\begin{example}[Discrete Example]
    Let \[p_X(x) = \begin{cases} \frac{1}{2} & x = 2 \\ \frac{1}{6} & x = 3 \\ \frac{1}{3} & x = 5. \end{cases}\]
    Then the corresponding transform is \[M(s) = E(e^{sx}) = \frac{1}{2} + \frac{1}{6} e^{3s} + \frac{1}{3} e^{5s}. \]
\end{example}

\begin{example}[Continuous Example]
    Let $X$ be an exponential RV with parameter $\lambda$: \[f_X(x) = \lambda e^{-\lambda x} \qquad x \geq 0.\]

    Then, \begin{align*}
        M(s) &= \lambda \int_{0}^{\infty} e^{sx} e^{-\lambda x} dx \\
        &= \lambda \int_{0}^{\infty} e^{(s - \lambda)x} dx \\
        &= \lambda \left(\frac{e^{(s - \lambda)x}}{s - \lambda}\right|_0^{\infty} \\
        &= \frac{\lambda}{\lambda - s}.
    \end{align*}
\end{example}

Notice, in above examples, that MGF is a \textbf{function} of parameter $s$, and not a number. We can also find MGF's for functions of $X$:

\begin{proposition}[MGF of Linear Function of RV]
    Let $Y = aX + b$. Then,
    \[M_Y(s) = E(e^{s(aX + b)}) = e^{sb} E(e^{saX}) = e^{sb} M_X(sa).\]
\end{proposition}

From our previous example, we see that $M_X(s) = \frac{1}{1 - s}$ where $X$ is the exponential distribution

\subsection{Moments}
Now that we've established what a moment generating function is, now it's time to understand what is being generated.

Let's do a generic MGF \[M(s) = \int_{-\infty}^{\infty} e^{sx} f_X(x) dx.\] Now, we take the derivative of this. 
\begin{align*} 
    \frac{d}{ds} M(s) &= \frac{d}{ds} \int_{-\infty}^{\infty} e^{sx} f_X(x) dx \\
    &= \int_{-\infty}^\infty \frac{d}{ds} e^{sx} f_X(x) dx \\
    &= \int_{-\infty}^\infty xe^{sx} f_X(x) dx.
\end{align*}

When $s = 0$, we have that this evaluates to $\int_{-\infty}^\infty xf_X(x) dx = E(X)$. If we differentiate $n$ times, then we will get \[\left(\frac{d^n}{ds^n} M(s)\right|_{s = 0} = \int_{-\infty}^{\infty} x^n f_X(x) dx = E(X^n).\]

\subsection{Inversion}
\begin{proposition}[Inversion Property]
    The MGF $M_X(s)$ associated with an RV $X$ uniquely determines the CDF of $X$, assuming that $M_X(s)$ is finite for all $s$ in some interval $[-a, a]$ for positive $a$. 
\end{proposition}

\subsection{Sum of Independent Random Variables}
\begin{proposition}
    Addition of independent random variables corresponds to multiplication of transforms. 
\end{proposition}

\begin{proof}
    Let $Z = X + Y$. $M_Z(s) = E(e^{sZ}) = E(e^{s(X+Y)}) = E(e^{sX}e^{sY}).$ Since $X, Y$ are independent, $e^{sX}$ and $e^{sY}$ are independent random variables for any fixed $s$. Thus, $E(e^{sX}e^{sY}) = E(e^{sX}) E(e^{sY}) = M_X(s)M_Y(s)$.
\end{proof}

We can further extend this; if $X_1, \ldots, X_n$ is a collection of independent random variables and $Z = X_1 + \cdots + x_n$, then $M_Z(s) = M_{X_1}(s) \cdots M_{X_n}(s)$. 

\section{Concentration Inequalities}
\begin{theorem}[Markov's Inequality]
	$P(X > a) = \dfrac{E(X)}{a}$.
\end{theorem}

\begin{theorem}[Chebyshev's Inequality]
	$P(|X - E(X)| > a) = \dfrac{\Var(X)}{a^2}$.
\end{theorem}

Used in lieu of confidence interval tests. 

\section{Modes of Convergence}
\subsection{Pointwise}
\begin{definition}[Pointwise Convergence]
	Fix $\omega \in \Omega$, $\{X_n(\omega)\}_{n=1}^\infty$ converges \vocab{pointwise} if it becomes a real-valued sequence. 
\end{definition}

Usually, people don't use this because of reasons highlighted in 104.
\subsection{Almost Sure}
\begin{definition}[Almost Sure Convergence] 
	$\{x_n\}_{n=1}^\infty$ converges \vocab{almost surely} to $X$ if $P(\{\omega : \omega \in \Omega, \lim_{n \to \infty} X_n(\omega) = X(\omega)\}) = 1$.
\end{definition}

This gets rid of $\omega$ with probability $0$. If you find an $\omega$ such that convergence doesn't hold, it's fine as long sa $P(\omega) = 0$. 

\subsubsection{Checking for Almost Sure Convergence}
There are a couple ways to check if some sequence converges almost surely.
\subsection{In Probability}
This is a weaker bound for convergence than almost sure convergence.

\subsection{In distribution}
\begin{definition}[In Distribution Convergence]
	$\{X_n\}_{n=1}^\infty$ converges in distribution (i.d.) to $X$ if for every $x \in \RR$, $P(X = x) = 0$.

	In other words, \[\lim_{n\to\infty} P(X_n \leq x) = 0.\] Denote this as $X_n \to^d x$.
\end{definition}

There are a couple of notable properties of in distribution convergence: 

\begin{theorem}
	In probability convergence implies in distribution convergence. 
\end{theorem}

\begin{proof}
	Suppose $X_n \to^P x$. 
\end{proof}

\subsection{Applications}
\subsubsection{Law of Large Numbers}
\begin{theorem}[Weak Law of Large Numbers]
	Let $\{X_n\}_{n-1}^\infty$ be independent and identically distributed (i.i.d) with finite mean $|E[X_1]| < \infty$. Then, 
	\[\overline{X_n} = \frac{X_1 + X_2 + \cdots + X_n}{n} \to^P E[X_1].\] 
\end{theorem}

\begin{proof}
	Recall Chebyshev's Inequality, which gives us \[P(|\bar{X_n} - E[\bar{X_n}]| \geq \epsilon) \leq \frac{E[(\bar{X_n} - E[\bar{X_n}]^2)]}{\epsilon^2}.\]

	Now, we calculate the variance: 
	\begin{align*} 
		\Var(\bar{X_n}) &= \Var\left(\frac{1}{n} (X_1 + X_2 + \cdots + X_n)\right) \\
		&= \frac{1}{n^2} \Var(X_1 + X_2 + \cdots + X_n) \\
		&= \frac{1}{n^2} (\Var(X_1) + \Var(X_2) + \Var(X_3) + \cdots + \Var(X_n)) \\
		&= \frac{\Var(X_1)}{n},
	\end{align*} 
	because $X_i$ are i.i.d.

	Applying Chebyshev gives us 
	\[\lim_{n \to \infty} P(|\bar{X_n} - E[X_1]| \geq \epsilon) \leq \lim_{n \to \infty} \frac{\Var(X_1)}{n \epsilon^2} = 0.\] 

	Thus, $\bar{X_n}$ converges in probability to $E[X_1]$. 
\end{proof}

The strong law of large numbers has the same claim, except instead of in probability convergence it's almost sure convergence. 

\subsubsection{Central Limit Theorem}
Once again let $\bar{X_n} = \frac{X_1 + X_2 + \cdots + X_n}{n}, S_n = X)1 + X_2 + \cdots + X_n$. Then, we know \[\Var(S_n) = n \Var(X_1) \to \infty.\]

We let $Z_n = \frac{S_n - n \mu}{\sigma \sqrt{n}}$.
\begin{theorem}[Central Limit Theorem]
	We have $\{X_n\}_{n=1}^\infty$ is i.i.d, with mean $\mu$ and variance $\sigma^2$. 

	Then, $Z_n \to^d \mathcal{N}(0, 1)$.
\end{theorem}

\begin{theorem}[Poisson Limit Theorem]
	Let $X_n = B(n \cdot \phi_n)$. Assume $\lim_{n \to \infty} n \cdot \phi_n = \lambda > 0$. Then, 
	\[X_n \to^d \mathrm{pois}(\lambda).\]
\end{theorem}

Now we see why normal and poisson distribs are so useful.
\end{document}

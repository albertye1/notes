\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage[portrait, margin=0.5in, top=0.8in, bottom=0.3in]{geometry}
\usepackage{amsmath, amssymb}
\setlength{\parskip}{1em}
\setlength{\parindent}{0cm} 
\usepackage[mdthm,simplethm]{test}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{physics}
\usepackage{sectsty}
\usepackage{tikz}
\usepackage{circuitikz}
\allsectionsfont{\normalfont\sffamily\bfseries}

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\pois}{\mathrm{pois}}
\setlength{\parindent}{0pt}
\pagestyle{fancy}
\lhead{EECS 126}
\rhead{Albert Ye}

\title{EECS 126}
\author{albert ye}
\date{\today}

\begin{document}
\maketitle
\section{Probability Space}
\subsection{Definition}
Essentially from 70. Events happen with some probability in a larger probability space that contains all events that can happen.
\subsection{Axioms of Probability}
\begin{proposition}[Axioms]
	\begin{enumerate}
		\item (Positivity) $P(\omega > 0)$ for any event $\omega$ in probability space $\Omega$. 
		\item (Totality) In any sample space $\Omega$, $P(\Omega) = 1$.
		\item (Additivity) If $A_1, A_2, \ldots, A_n$ are independent, then \[\sum_{i=1}^n A_i = \bigcup_{i = 1}^n A_i.\] 
	\end{enumerate}
\end{proposition}

From just this, we can get some useful information, such as the union bound.
\begin{theorem}[Union Bound]
	\[P\left(\bigcup_{i = 1}^n A_i\right) = \sum_{i = 1}^n P(A_i).\]
\end{theorem}

The proof is left as an exercise to the student, probably in the homework.
\subsection{$\sigma$-algebra}
\begin{definition}[$\sigma$-algebra]
	Given a sample space $\Omega$, a set $\mathcal{F} \subseteq 2^{\Omega}$ is a $\sigma$-algebra if:
	\begin{enumerate}
		\item $\Omega \in \mathcal{F}$
		\item If any event $A$ is in $\mathcal{F}$, then its complement $\Omega \setminus A$ is also in $\mathcal{F}$.
		\item For countably many events $A_1, A_2, \ldots, A_n, \ldots \in \mathcal{F}$, their union $A = \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
	\end{enumerate}
\end{definition}

The biggest note is that $\Omega$ must be in a $\sigma$-algebra in order for any of the axioms of probability to apply.
\newpage
\section{Conditional Probability}
\subsection{Definition}
\subsection{Total Probability}
\subsection{Bayes' Rule}
\subsection{Continuous Bayes}

\newpage
\section{It Depends}
\subsection{Independence / (Un)correlation}
\subsection{Conditional Expectation}
Notice that $E[X | Y]$ is a random variable, but $E[X | Y = y]$ is a number. We can call $E[X | Y]$ a function $g(Y)$, where then $E[X | Y = y] = g(y)$ is just a value in the function.
\subsection{Iterated Expectation}

\newpage
\section{Distributions}
\subsection{Joint Distribution}
\begin{definition}[Joint Distribution]
	A joint distribution $f_{X, Y}(x, y)$ 
\end{definition}
\subsection{Marginal Distribution}
\subsection{Derived Distribution}

\newpage
\section{Random Variables}
\subsection{Discrete}
\subsubsection{Bernoulli}
\begin{itemize}
	\item PMF: $p_X(k) = \begin{cases}p & k = 1 \\ 1=p & k = 0\end{cases}$
	\item Expected value: $p$
	\item Variance: $p(1 - p)$.
\end{itemize}
\subsubsection{Binomial}
\begin{itemize}
	\item PMF: $p_X(k) = \binom{n}{k} p^k(1 - p)^{n - k}$ over all $k \in 0, 1, \ldots, n$.
	\item Expected value: $np$
	\item Variance: $np(1 - p)$.
\end{itemize}

Run a Bernoulli test $n$ times, find how many are positive.
\subsubsection{Geometric}
\begin{itemize}
	\item PMF: $p_X(k) = (1 - p)^{k - 1} p$, for $k = 1, 2, \ldots$.
	\item Expected value: $\frac{1}{p}$
	\item Variance: $\frac{1 - p}{p^2}$.
\end{itemize}

Here, each trial has a $p$ probability of success, and we want to find the \# of trials until one success.
\subsubsection{Poisson}
\begin{itemize}
	\item PMF: $p_X(k) = \frac{\lambda^k(e^{-\lambda})}{k!}$.
	\item Expected value: $\lambda$
	\item Variance: $\lambda$
\end{itemize}

Used to simulate arrivals, I guess. More useful later, with Poisson processes.
\subsection{Continuous}
\subsubsection{Uniform}
\subsubsection{Exponential}
\subsubsection{Gaussian}
\subsubsection{Joint Gaussian}
The main tips for Joint Gaussian are to approach it as a sort of vectorized Gaussians over a certain number $N$ of dimensions. 
Most of the addition / whatever operations in a Gaussian can be remodeled as a Joint Gaussian.

\section{Moment Generating Functions}
\begin{definition}
    The \vocab{moment generating function} (also known as a transform) associated with a RV $X$, is a function $M_X(s)$ of a scalar parameter $s$ defined by $M_X(s) = E(e^{sX})$.
\end{definition}

the simpler notation $M(S)$ can be used whenever the underlying random variable $X$ is clear from context. In more detail, when $X$ is a discrete random variable, the corresponding MGF is given by \[M(s) = \sum_{x} e^{sx} p_X(x).\] Analogously, when continuous, we just replace the summation with an integral to get \[M(s) = \int_{-\infty}^{\infty} e^{sx} f_X(x) dx.\]

Just an example so that I know what the reference is here:
\begin{example}[Discrete Example]
    Let \[p_X(x) = \begin{cases} \frac{1}{2} & x = 2 \\ \frac{1}{6} & x = 3 \\ \frac{1}{3} & x = 5. \end{cases}\]
    Then the corresponding transform is \[M(s) = E(e^{sx}) = \frac{1}{2} + \frac{1}{6} e^{3s} + \frac{1}{3} e^{5s}. \]
\end{example}

\begin{example}[Continuous Example]
    Let $X$ be an exponential RV with parameter $\lambda$: \[f_X(x) = \lambda e^{-\lambda x} \qquad x \geq 0.\]

    Then, \begin{align*}
        M(s) &= \lambda \int_{0}^{\infty} e^{sx} e^{-\lambda x} dx \\
        &= \lambda \int_{0}^{\infty} e^{(s - \lambda)x} dx \\
        &= \lambda \left(\frac{e^{(s - \lambda)x}}{s - \lambda}\right|_0^{\infty} \\
        &= \frac{\lambda}{\lambda - s}.
    \end{align*}
\end{example}

Notice, in above examples, that MGF is a \textbf{function} of parameter $s$, and not a number. We can also find MGF's for functions of $X$:

\begin{proposition}[MGF of Linear Function of RV]
    Let $Y = aX + b$. Then,
    \[M_Y(s) = E(e^{s(aX + b)}) = e^{sb} E(e^{saX}) = e^{sb} M_X(sa).\]
\end{proposition}

From our previous example, we see that $M_X(s) = \frac{1}{1 - s}$ where $X$ is the exponential distribution

\subsection{Moments}
Now that we've established what a moment generating function is, now it's time to understand what is being generated.

Let's do a generic MGF \[M(s) = \int_{-\infty}^{\infty} e^{sx} f_X(x) dx.\] Now, we take the derivative of this. 
\begin{align*} 
    \frac{d}{ds} M(s) &= \frac{d}{ds} \int_{-\infty}^{\infty} e^{sx} f_X(x) dx \\
    &= \int_{-\infty}^\infty \frac{d}{ds} e^{sx} f_X(x) dx \\
    &= \int_{-\infty}^\infty xe^{sx} f_X(x) dx.
\end{align*}

When $s = 0$, we have that this evaluates to $\int_{-\infty}^\infty xf_X(x) dx = E(X)$. If we differentiate $n$ times, then we will get \[\left(\frac{d^n}{ds^n} M(s)\right|_{s = 0} = \int_{-\infty}^{\infty} x^n f_X(x) dx = E(X^n).\]

\subsection{Inversion}
\begin{proposition}[Inversion Property]
    The MGF $M_X(s)$ associated with an RV $X$ uniquely determines the CDF of $X$, assuming that $M_X(s)$ is finite for all $s$ in some interval $[-a, a]$ for positive $a$. 
\end{proposition}

\subsection{Sum of Independent Random Variables}
\begin{proposition}
    Addition of independent random variables corresponds to multiplication of transforms. 
\end{proposition}

\begin{proof}
    Let $Z = X + Y$. $M_Z(s) = E(e^{sZ}) = E(e^{s(X+Y)}) = E(e^{sX}e^{sY}).$ Since $X, Y$ are independent, $e^{sX}$ and $e^{sY}$ are independent random variables for any fixed $s$. Thus, $E(e^{sX}e^{sY}) = E(e^{sX}) E(e^{sY}) = M_X(s)M_Y(s)$.
\end{proof}

We can further extend this; if $X_1, \ldots, X_n$ is a collection of independent random variables and $Z = X_1 + \cdots + x_n$, then $M_Z(s) = M_{X_1}(s) \cdots M_{X_n}(s)$. 

\section{Concentration Inequalities}
\begin{theorem}[Markov's Inequality]
	$P(X > a) = \dfrac{E(X)}{a}$.
\end{theorem}

\begin{theorem}[Chebyshev's Inequality]
	$P(|X - E(X)| > a) = \dfrac{\Var(X)}{a^2}$.
\end{theorem}

Used in lieu of confidence interval tests. 

\section{Modes of Convergence}
\subsection{Pointwise}
\begin{definition}[Pointwise Convergence]
	Fix $\omega \in \Omega$, $\{X_n(\omega)\}_{n=1}^\infty$ converges \vocab{pointwise} if it becomes a real-valued sequence. 
\end{definition}

Usually, people don't use this because of reasons highlighted in 104.
\subsection{Almost Sure}
\begin{definition}[Almost Sure Convergence] 
	$\{x_n\}_{n=1}^\infty$ converges \vocab{almost surely} to $X$ if $P(\{\omega : \omega \in \Omega, \lim_{n \to \infty} X_n(\omega) = X(\omega)\}) = 1$.
\end{definition}

This gets rid of $\omega$ with probability $0$. If you find an $\omega$ such that convergence doesn't hold, it's fine as long sa $P(\omega) = 0$. 

\subsubsection{Checking for Almost Sure Convergence}
There are a couple ways to check if some sequence converges almost surely.
\subsection{In Probability}
This is a weaker bound for convergence than almost sure convergence.

\subsection{In distribution}
\begin{definition}[In Distribution Convergence]
	$\{X_n\}_{n=1}^\infty$ converges in distribution (i.d.) to $X$ if for every $x \in \RR$, $P(X = x) = 0$.

	In other words, \[\lim_{n\to\infty} P(X_n \leq x) = 0.\] Denote this as $X_n \to^d x$.
\end{definition}

There are a couple of notable properties of in distribution convergence: 

\begin{theorem}
	In probability convergence implies in distribution convergence. 
\end{theorem}

\begin{proof}
	Suppose $X_n \to^P x$. 
\end{proof}

\subsection{Applications}
\subsubsection{Law of Large Numbers}
\begin{theorem}[Weak Law of Large Numbers]
	Let $\{X_n\}_{n-1}^\infty$ be independent and identically distributed (i.i.d) with finite mean $|E[X_1]| < \infty$. Then, 
	\[\overline{X_n} = \frac{X_1 + X_2 + \cdots + X_n}{n} \to^P E[X_1].\] 
\end{theorem}

\begin{proof}
	Recall Chebyshev's Inequality, which gives us \[P(|\bar{X_n} - E[\bar{X_n}]| \geq \epsilon) \leq \frac{E[(\bar{X_n} - E[\bar{X_n}]^2)]}{\epsilon^2}.\]

	Now, we calculate the variance: 
	\begin{align*} 
		\Var(\bar{X_n}) &= \Var\left(\frac{1}{n} (X_1 + X_2 + \cdots + X_n)\right) \\
		&= \frac{1}{n^2} \Var(X_1 + X_2 + \cdots + X_n) \\
		&= \frac{1}{n^2} (\Var(X_1) + \Var(X_2) + \Var(X_3) + \cdots + \Var(X_n)) \\
		&= \frac{\Var(X_1)}{n},
	\end{align*} 
	because $X_i$ are i.i.d.

	Applying Chebyshev gives us 
	\[\lim_{n \to \infty} P(|\bar{X_n} - E[X_1]| \geq \epsilon) \leq \lim_{n \to \infty} \frac{\Var(X_1)}{n \epsilon^2} = 0.\] 

	Thus, $\bar{X_n}$ converges in probability to $E[X_1]$. 
\end{proof}

The strong law of large numbers has the same claim, except instead of in probability convergence it's almost sure convergence. 

\subsubsection{Central Limit Theorem}
Once again let $\bar{X_n} = \frac{X_1 + X_2 + \cdots + X_n}{n}, S_n = X)1 + X_2 + \cdots + X_n$. Then, we know \[\Var(S_n) = n \Var(X_1) \to \infty.\]

We let $Z_n = \frac{S_n - n \mu}{\sigma \sqrt{n}}$.
\begin{theorem}[Central Limit Theorem]
	We have $\{X_n\}_{n=1}^\infty$ is i.i.d, with mean $\mu$ and variance $\sigma^2$. 

	Then, $Z_n \to^d \mathcal{N}(0, 1)$.
\end{theorem}

\begin{theorem}[Poisson Limit Theorem]
	Let $X_n = B(n \cdot \phi_n)$. Assume $\lim_{n \to \infty} n \cdot \phi_n = \lambda > 0$. Then, 
	\[X_n \to^d \mathrm{pois}(\lambda).\]
\end{theorem}

Now we see why normal and poisson distribs are so useful.
\newpage 
\section{Information Theory}
\subsection{Entropy}
First, we define $\mathcal{X}$ as the range of a random variable $X$ over all events in a probability space. 

\begin{definition}[Entropy] 
	Given a discrete random variable $X$ and PMF $P_X(x)$, we have \vocab{entropy}
	\[H(X) = \sum_{x \in \mathcal{X}} P_X(x) \log \frac{1}{P_X(x)}.\] 
\end{definition}

Furthermore, the average amount of surprise is defined as $E\left[\log \frac{1}{P_X(x)}\right]$.

Moreover, some properties of entropy: 
\begin{enumerate}
	\item $H(X) \geq 0$
	\item $H(X)$ is 
	\item $H(X) \leq \log |x|$, achieved when $X$ is uniform on $x$.
\end{enumerate}

Where $x$ is the range of $X(\omega)$ for all $\omega \in \Omega$.

\begin{definition}[Joint Entropy]
	Joint entropy $(X, Y) \sim P_{X, Y}$:
	\[H(X, Y) = \sum_{(x, y) \in \mathcal{X} \times \mathcal{Y}} P_{X, Y} (x, y) \log \frac{1}{P_{X, Y}(x, y)}.\]
\end{definition}

\begin{definition}[Conditional Entropy] 
	\[H(Y | X) = \sum_{x \in \mathcal{X}} H(Y | X = x).\] 
\end{definition}

Next, we observe some properties of joint and conditional entropy. 

\begin{proposition} 
	\begin{enumerate}
		\item (Chain Rule) \[H(X, Y)  H(X) + H(Y | X) = H(Y) + H(X | Y).\] 
		\item (Conditioning Reduces Entropy) \[H(Y | X) \leq H(Y).\]
		\item \[H(X, Y) \leq H(X) + H(Y).\] 
	\end{enumerate}
\end{proposition}

\subsection{Mutual Information}
Created by a Bob Fano, who argued more important than entropy.

\begin{definition}[Mutual Information]
	We define $I(X, Y)$ as the \vocab{mutual information} between $X$ and $Y$, such that
	\begin{align*}
		I(X: Y) &= H(X) - H(X | Y) \geq 0 \\
				&= H(X) + H(Y) - H(X, Y) \\
				&= H(Y) - H(Y | X).
	\end{align*}
\end{definition}

We can think of $I(X, X) = H(X)$ as well.

\begin{definition}[Kullback-Leibler Divergence] 
	We can also call this \vocab{relative entropy}. 

	\[D(P \parallel Q) = \sum_{x \in \mathcal{X}} P(X) \log \frac{P(x)}{Q(x)} \geq 0.\]
\end{definition}

We can see that the mutual information can further be reduced to 
\begin{align*}
	I(X : Y) &= \sum_{(x, y) \in \mathcal{X} \times \mathcal{Y}} P_{X, Y}(x, y) \log \frac{P_{X, Y}(x, y)}{P_X(x)P_Y(y)} \\
			 &= D(P_{X, Y} \parallel P_X \otimes P_Y),
\end{align*}
where we define $P_X \otimes P_Y$ as the cross product. 

\subsection{Source Coding}
	Let $X_1, X_2, \ldots, X_n$ be a string of symbols or binary code or etc. in a file. We want to convert this into some compressed $b(X_1, X_2, \ldots X_n)$.
\begin{theorem}
	We assume $X_1, X_2, \ldots, X_n$ are i.i.d as $X$.
	\begin{enumerate}
		\item There exists a source code such that 
			\[\lim_{n \to \infty} E\left[\frac{1}{n} |b(x_1, \cdots, x_n)|\right] \leq H(X) + \epsilon\] for any $\epsilon > 0.$ 

		\item Conversely, no source code can achieve an average length less than $H(X)$ bits per symbol. 
	\end{enumerate}
\end{theorem}

\section{Markov Chains}
\begin{definition}[Markov Chain]
	$\{X_n\}_{n \in \NN}$ is a discrete-time Markov Chain (DTMC) on state space $\mathcal{X}$ if it satisfies the Markov property: For all positive integers $n$ and feasible sequence of states $x_0, x_1, x_2, \ldots, x_{n+1} \in \mathcal{X}$; 
	\[\Pr(X_{n+1} = x_{n+1} | X_n = x_n, X_{n-1} = x_{n-1}, \ldots, X_0 = x_0) = P(X_{n+1} = x_{n+1}, X_n = x_n).\]
\end{definition}

We further denote $P$ as the transition probability matrix, which is done by taking the row statistic of $\mathcal{X}$.

\subsection{Distributions}
Denote distribution of $X_n$ as $\Pi_n$. Then, $\Pi_n = \Pi_0 P^n$. We have a \vocab{stationarity distribution} $\Pi = \Pi \cdot P$, and this is also called the balance equation. 

\subsection{Recurrence and Transience}
For $x \in \mathcal{X}$, we define $T_x = \min \{n \in \NN, X_n = n\}$ as the hitting time of $x$, and $T_x^+ = \min \{n \in \ZZ_+ X_n = n\}$.

$T_x$ determines the first time that a Markov chain reaches a certain state, and $T_x^+$ calculates the same thing except ignoring trivial (initial) cases.

Now, some notation.
Let $\Pr_x(A) = \Pr(A | X_0 = x)$ and $E_x[Z] = E[Z | X_0 = x]$. This is probability and expectation given an initial state in the Markov chain. Furthermore, let $\rho_{x, y} = \Pr_x(T_y^+ < \infty)$, $\rho_x = \rho_{x, x}$. 

\begin{definition}
	State $x$ is \vocab{recurrent} if $\rho_x = 1$, \vocab{transient} otherwise.

	A recurrent state essentially means that a state in a Markov chain will certainly be reached again.
\end{definition}

\begin{proposition}
	Denote $N_x = \sum_{n \in \NN} \mathbb{I}(X_n = x)$. Then, 
	\begin{enumerate}
		\item If $x$ is recurrent, then $N_x = \infty$ almost surely.
		\item If $x$ is transient then $E_x[N_x] = \frac{\rho(x)}{1 - \rho(x)}$. 
	\end{enumerate}
\end{proposition}

\subsection{Classification of States}
\begin{definition}[Communicating Class]
We say $x$ communicates with $y$ if $\rho_{x, y} > 0$ and $p_{y, x} > 0$. 

A \vocab{communicating class} is a maximal set of states which communicate with each other.
\end{definition}

\begin{definition}
	Markov Chain is \vocab{irreducible} if it consists of only a single communicating class.  
\end{definition}

The class property is a property that's necessarily shared by all members of class. Anyways, now time to start applying the many definitions we've just made:

\begin{theorem}
	Recurence and transience are class properties.
\end{theorem}

Are we not going over the proof for this?

\begin{proposition}
	Every finite state irreducible chain is recurrent.
\end{proposition}

\begin{proof}
	Basically prove that one of the states must be recurrent using the fact that there are finite states, and then use the above theorem to see that this is a class property.
\end{proof}

\subsection{Big Theorem}
\begin{theorem} 
	Suppose a markov chain is irreducible with a stationary distribution $\Pi$. Then, 
	\[\Pi(x) = \frac{1}{\EE_X[T_X^+]}\] 
\end{theorem}

To prove this, we introduce another claim.

\begin{theorem} 
	Suppose a Markov chain is irreducible, aperiodic, and has stationary distribution $\Pi$. Then, as $n \to \infty$, $P_n(x, y) \to \Pi(y)$ for all $x, y$.
\end{theorem}

The \textbf{aperiodic} assumption is correct, because if the result is periodic, then it is clear to see that this convergence is not true. 

Moreover, $P_n(x, y) = \Pr(X_n = y \mid x_0 = x)$.

\begin{proof} 
	Let $\mathcal{X}^2 = \mathcal{X} \times \mathcal{X}$, and we define a new transition probability $\bar{P}$ on $\mathcal{X}^2$. 
	Then, we define $\bar{P}((x_1, y_1), (x_2, y_2)) = P(x_1, x_2) P(y_1, y_2)$. We claim that $\bar{P}$ is irreducible.

	Since $P$ is irreducible, there exist $K, L$ such that $P_K(x_1, x_2) > 0$, $P_L(y_1, y_2) > 0$. 

	\begin{lemma}
		For irreducible aperiodic Markov Chain there exists $m_0$ such that $P_m(x, x) > 0$ for all $m > m_0$, where $m_0$ depends on $x$.
	\end{lemma}

	\begin{proof} 
		\begin{align*}
			\Pr(X_n = y, T \leq n) &= \sum_{m = 1}^n \sum_{x} \Pr(T = m, X_m = x, Y_n = y) \\
								   &= \sum_{m = 1}^n \sum_{x} \Pr(T = m, X_m = x) P(Y_n = y | X_m = x, T = m) \\
								   &= \sum_{m = 1}^n \sum_{x} \Pr(T = m, X_m = x) P(Y_n = y | Y_m = x) \\
								   &= \Pr(Y_n = y, T \leq n).
		\end{align*}

		We can extend the Markov property here by applying it recursively to state that conditioned on some event $X_m$ in Markov chain, a future event $X_n$ is conditionally independent of \textbf{all} past events $(X_0, \ldots, X_{m-1})$.
	\end{proof}

	Using the aperiodicity lemma, we know that for $M$ large enough, $P_{K+M}(x_1, x_2) > 0$, and $P_{L+M}(y_1, y_2) > 0$. It then follows that $\bar{P}_{K+L+M}((x_1, y_1), (x_2, y_2)) > 0$.  

	I honestly am completely lost for the rest of the proof I'll figure it out later...
\end{proof}

\subsection{Reversibility}
Asking the following question: Does the Markov Chain still work when played in reverse? 

We let $(Y_0, Y_1, \ldots, Y_n) \equiv (X_n, X_{n-1}, \ldots, X_0)$. 

\begin{lemma} 
	$Y_n$ is still a Markov Chain with transition matrix $\hat{P}$, where 
	\[\hat{P}(x, y) = \frac{\Pi(y) P(y, x)}{\Pi(x)}.\] 
\end{lemma}

\begin{definition}[Reversibility] 
	We say that a Markov chain is \vocab{reversible} if $\hat{P} = P$.

	The detailed balance equation states that $\Pi(x) P(x, y) = \Pi(y) P(y, x)$.
\end{definition}

\section{Poisson Process}
This is based on exponential distributions having the memoryless property. 

So if we have $X \sim \mathrm{Exp}(\lambda)$, then $f_X(x) = \lambda e^{-\lambda x}, x \geq 0$, then the CDF is $P(x > t) = e^{-\lambda t}$. 
We then have the following property: $\Pr(X > t + s \mid X > t) = \Pr(X > s)$. 

\begin{definition}[Poisson Process] 
Fix $\lambda > 0$. Assume that inter-arrival times $s_1, s_2, \ldots$ are i.i.d. $\mathrm{Exp}(\lambda)$. For each $x \geq 1$, define \[T_n = \sum_{j = 1}^n S_j, T_0 = 0.\]
Moreover, \[N(t) = \max(n > 0 : T_n \leq t).\] We call the continuous time stochastic process $\{N(t)\}_{t > 0}$ the \vocab{Poisson process} $\mathrm{PP}(\lambda)$.
\end{definition}

Next, we define the Big Theorem of Poisson processes with its primary key properties.
\begin{definition}[Increment of Poisson Process] 
	\[N(T_1, T_2) = N(_2) - N(T_1), T_2 \geq T_1.\]
\end{definition}

\begin{theorem}[Big Theorem]
	$\qquad$
	\begin{enumerate}
		\item \textbf{Stationary Increment.} $N(t, t + s)$ has the same distribution as $N(s)$.
		\item \textbf{Independent Increment.} For $0 < t_1 < t_2 < \ldots < t_k$ the set of random variables $N(t_1), N(t_1, t_2), \ldots, N(t_{k-1}, t_k)$ are jointly independent.
		\item $N(t) \sim \pois(\lambda t)$.
	\end{enumerate}
\end{theorem}

We can generalize Poisson processes into multiple dimensions with the Poisson random field, so $\pois(\int_{A} \lambda)$ finds the Poisson process over arrivals in a region $A$.

\begin{definition}[Erlang Distribution] 
	\[f_{T_n}(t) = \frac{\lambda^n t^{n-1} e^{\lambda t}}{(n-1)!}\]
\end{definition}

There are two ways to look at a Poisson process: the counts and the inter-arrivals.

\begin{theorem}
	Let $S_1, S_2, \ldots$ be some set of almost surely positive inter-arrival times and define $T_n = \sum_{j=1}^n S_j$, $N(t) = \max(n \geq 0 : T_n \leq t)$.

	If $\{N(t)\}_{t \geq 0}$ has stationary independent increments, and $N(t) \sim \pois(\lambda t)$, then $S_1, S_2, \ldots$ are i.i.d. $\mathrm{Exp}(\lambda)$ random variables.
\end{theorem}

\begin{definition}[Splitting] 
	$N \sim \mathrm{PP}(\lambda)$, and $B_1, B_2, \ldots \sim \mathrm{Bern}(p)$. 

	The splitting process essentially assigns to one of $N_0$ or $N_1$. 
	\[N_0(t) = |\{ i : B_i = 0, i \leq N(t)\}|\] 
	\[N_1(t) = |\{ i : B_i = 1, i \leq N(t)\}|\]
	Then $N_1 \sim \mathrm{PP}(\lambda p), N_0 \sim \mathrm{PP}(\lambda(1 - p))$, so without being given any knowledge of $N$, we have that $N_0, N_1$ are independent.  
\end{definition}

Essentially what is happening is that when something arrives, we flip a (weighted) coin and flip a switch to determine which process it actually reaches.
\end{document}
